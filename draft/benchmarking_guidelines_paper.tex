%%%%%%%%%%%%%%%%%%%
% Document settings
%%%%%%%%%%%%%%%%%%%

\documentclass[12pt, a4paper]{article}
\usepackage[top = 2.25cm, left = 2.25cm, right = 2.25cm, bottom = 2.25cm]{geometry}
\usepackage[utf8]{inputenc}

\usepackage{setspace}  % line spacing
\usepackage{lineno}  % line numbers

%\usepackage{cite}  % citations
\usepackage[square, numbers]{natbib}

\usepackage{float}  % figure/table placement
\usepackage{graphicx}  % figures

% disable ligatures
\usepackage{microtype}
\DisableLigatures[f]{encoding = *, family = *}

% include dots in table of contents
%\usepackage{tocloft}
%\renewcommand{\cftsecleader}{\cftdotfill{\cftdotsep}}

% avoid single lines of text at top or bottom of page
\widowpenalty 10000
\clubpenalty 10000

% text layout
\sloppy
\setlength{\parindent}{1cm}

% captions: bold 'Figure #' heading and end with full stop; left-justified
\usepackage[aboveskip = 10pt, labelfont = bf, labelsep = period, singlelinecheck = off, font = small]{caption}

% links
\usepackage{hyperref}
\hypersetup{colorlinks, citecolor = black, linkcolor = black, urlcolor = blue}

\usepackage[shortcuts]{extdash}  % allow line breaks at hyphens

% subfigures
\usepackage{subcaption}

% leave date blank
%\date{}

% adjust font style for section headings
\usepackage{titlesec}
\titleformat*{\section}{\large\bfseries\flushleft}

% to adjust indent width in enumerated list
\usepackage{enumitem}



 When designing custom metrics, it is important to assess their robustness and usefulness across a wide range of possible predictions.
%%%%%%%%%%
% Document
%%%%%%%%%%

\begin{document}




%%%%%%%%%%%%
% Title page
%%%%%%%%%%%%

\title{\Large{\textbf{
Essential Guidelines for Method Benchmarking
}}{\vskip 0.75cm}}

\author{
Lukas M. Weber$^{1,2}$,
Wouter Saelens$^{3,4}$,\\
Robrecht Cannoodt$^{3,4}$,
Charlotte Soneson$^{1,2}$,\\
Yvan Saeys$^{3,4,*}$,
Mark D. Robinson$^{1,2,*}$\\ \\
\normalsize{$^1$Institute of Molecular Life Sciences, University of Zurich, }\\
\normalsize{Zurich, Switzerland}\\
\normalsize{$^2$SIB Swiss Institute of Bioinformatics, University of Zurich, }\\
\normalsize{Zurich, Switzerland}\\
\normalsize{$^3$Data Mining and Modelling for Biomedicine, VIB Center for Inflammation Research, }\\
\normalsize{Ghent, Belgium}\\
\normalsize{$^4$Department of Applied Mathematics, Computer Science and Statistics, Ghent University, }\\
\normalsize{Ghent, Belgium}\\ \\
\normalsize{$^\star$Corresponding authors}\\ \\ \\
}

\date{\today}



\begin{titlepage}

\maketitle

% no page number (note: this line must be after \maketitle)
\thispagestyle{empty}

\end{titlepage}




%%%%%%%%%%%%%%%%%%%
\section*{Abstract}
%%%%%%%%%%%%%%%%%%%

In computational biol When designing custom metrics, it is important to assess their robustness and usefulness across a wide range of possible predictions.ogy and other sciences, researchers are frequently faced with a choice between multiple available methods for performing computational data analyses. Method benchmarking studies aim to rigorously compare and evaluate the performance of different methods using representative benchmark datasets, in order to provide recommendations for the most suitable choice of method in a given analysis setting. However, method benchmarking studies must be carefully designed and implemented in order to provide accurate and unbiased results. Here, we provide several key guidelines and recommendations for performing high-quality method benchmarking, based on our own experiences in computational biology.




%%%%%%%%%%%%%%%%%%%%%%%
\section*{Introduction}
%%%%%%%%%%%%%%%%%%%%%%%

Many fields of computational research are characterized by a fast growing number of available methods for data analysis. For example, as of mid 2018, more than 230 methods are available for analyzing data from single-cell RNA-sequencing experiments \citep{Zappia2018}. For experimental researchers and other users of methods, this represents an invaluable opportunity, since new methods are frequently designed to address limitations of existing methods; e.g., to improve statistical performance or to expand the types of analyses that may be considered. From the perspective of the end user, selecting the optimal method for a given analysis is a crucial decision, which can significantly affect results and conclusions.

Method benchmarking or comparison studies are carried out by computational researchers in order to compare the performance of different methods for the same analysis task, using standardized benchmark datasets and a range of evaluation criteria. These studies may be carried out by authors of new methods to demonstrate performance improvements or other advantages of their approach, or by independent groups interested in systematically comparing existing methods. In our view, benchmarking studies by independent groups are particularly valuable for the research community, since there is no perception of bias: there is no perceived incentive to show that the authors' own method performs best.

Our research groups have recently performed several benchmarking studies in computational biology \citep{Saelens2018a, Saelens2018b, Duo2018, Soneson2018, Weber2016}. During these studies, we have learned a number of key lessons. In this review, we provide a series of essential guidelines and recommendations for performing method benchmarking, based on our experiences, which we hope will assist other computational researchers to design and implement rigorous, informative, and unbiased method benchmarkings. Our perspectives are naturally influenced by our domain experiences in computational biology, and at several points we use examples from computational biology to illustrate our recommendations; however, we expect that our major arguments apply equally to other fields. Box 1 summarizes our main recommendations, and each subsequent section expands on one of these recommendations.




% Box

\vskip 1cm

\setlength{\fboxsep}{4mm}
\noindent\fbox{\begin{minipage}{0.95\textwidth}
\begin{flushleft}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Box 1: Summary of guidelines}
%%%%%%%%%%%%%%%%%%%%%%%%% When designing custom metrics, it is important to assess their robustness and usefulness across a wide range of possible predictions.%%%%%%%%%%%%%%

The essential guidelines presented in this review may be summarized in the form of the following series of rules or recommendations. Each recommendation is discussed in more detail in the corresponding section in the text.

\begin{enumerate}[leftmargin = 8mm]

\item Define the purpose and scope of your benchmarking or comparison.
\item Ensure that you have included all relevant methods.
\item Select (or design) a variety of representative benchmark datasets.
\item Evaluate and rank methods according to quantitative performance metrics.
\item Compare runtimes and computational requirements, as well as user-friendliness, documentation, and other qualitative measures.
\item Record all parameter settings, as well as environment or session information.
\item Consider your audience: interpret results and provide conclusions or guidelines from both user and method developer perspectives.
\item Publish and distribute your results in an accessible format.
\item Design the benchmarking to allow future extensions to include new methods or new datasets.
\item Follow reproducible research best practices by making all code and data available in public repositories.

\end{enumerate}

\end{flushleft}
\end{minipage}}

\vskip 1.5cm




%%%%%%%%%%%%%%%%%%%%%%%%%%% When designing custom metrics, it is important to assess their robustness and usefulness across a wide range of possible predictions.%%%%%%%%%%%%%%
\section*{Defining the purpose and scope}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Method benchmarking may be performed for a variety of reasons, depending on the study context. The underlying purpose of the method benchmarking or comparison will fundamentally guide the design and implementation. The purpose and scope should be clearly defined at the beginning of the study, to ensure that results are meaningful and that time and effort are not lost on unnecessary work.

In general, we can define two broad types of benchmarking studies: (i) method benchmarking performed during the introduction of a new method, in order to demonstrate the merits of the authors' new method (e.g., \citep{Weber2018, Nowicka2016, Levine2015, Zhou2014, Law2014}); or (ii) independent benchmarking studies performed by external groups to systematically compare all (or a large number of) available methods for a certain type of analysis (e.g., \citep{Saelens2018a, Saelens2018b, Duo2018, Soneson2018, Weber2016}).

A key question to consider is how detailed the benchmarking should be. For an independent benchmarking, a high level of detail across all methods is crucial, including evaluation according to defined performance metrics as well as other measures such as user-friendliness and documentation quality. Ideally, the evaluations should test a range of values for major parameters for each method. By contrast, when introducing a new method, the focus of the benchmarking will be on evaluating and comparing the performance of the new method; this may be sufficiently achieved by comparing against a single set of optimal parameter values (as judged during initial exploratory work) for competing methods.

Depending on the scope of the benchmarking, it may be helpful to formulate hypotheses and tests for the most important statements that the benchmarking is designed to address. For large-scale benchmarking efforts, it may be beneficial to involve method developers directly, either through benchmarking competitions or other community initiatives (e.g., the FlowCAP \citep{Aghaeepour2013, Aghaeepour2016} and DREAM \citep{Eduati2015} challenges), or by soliciting feedback. Finally, results should be carefully interpreted to ensure that they correctly address the key questions and purpose of the study, from the perspective of the intended readers (method users or developers). For example, for an independent benchmarking: from the perspective of method users, does one method perform best overall, or (more commonly) do several different methods achieve top performance for slightly different types of data? From the perspective of developers, what could be improved in future methods?




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Selection of methods}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

For any benchmarking, a thorough review of the literature to find all relevant methods is essential. For an independent benchmarking, as many methods as possible should be included, to provide readers with comprehensive information. In this case, the publication describing the benchmarking will also function as a review of the literature, so it will be useful to include information characterizing and describing each method; e.g., in the form of a summary table (see Table 1 in \citep{Weber2016} or Table 1 in \citep{Saelens2018b}). Defining formal inclusion criteria is helpful to avoid the perception of bias: e.g., all methods that (i) provide freely available software implementations, (ii) are available for certain operating systems or programming language environments, and (iii) can successfully be installed without errors. Alternatively, the benchmarking could be designed as a competition or other community initiative, inviting method developers to run their methods directly (e.g., the FlowCAP \citep{Aghaeepour2013, Aghaeepour2016} and DREAM \citep{Eduati2015} challenges).

When introducing and benchmarking a new method, a smaller selection of competing methods may be sufficient. This should include (if known) the best-performing method(s) for this type of analysis, a simple `baseline' method (e.g., k-means clustering, linear discriminant analysis, logistic regression, etc.), any methods that are particularly popular in the literature, and possibly (if applicable) methods representing distinct underlying mathematical approaches. The selection of competing methods should ensure an accurate and unbiased assessment of the relative strengths and weaknesses of the new approach, compared to the current `state-of-the-art'.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Selection (or design) of benchmark datasets}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The selection of benchmark datasets is a critical design choice. If suitable publicly available datasets do not already exist, they will need to be generated or constructed, either experimentally or by simulation. Depending on the complexity, this may become the most conceptually demanding part of the benchmarking analysis. A variety of benchmark datasets with different characteristics should be included, to investigate the performance of methods under widely varying conditions.

There are two main types of benchmark datasets: simulated (or synthetic) data and experimental (or real) data. Simulated data provide the advantage that a known `truth' (or `ground truth') can easily be included; e.g., differentially expressed genes, or differentially abundant cell populations. This allows performance metrics to be calculated, which can then be used to rank methods by their ability to recover the known truth. However, care must be taken to design a meaningful simulation, which accurately reflects the properties of real experimental data; e.g., this can be achieved by first inspecting the multi-dimensional distributions of experimental datasets. Simulations should not be too `simple', since this could lead to all methods performing perfectly or equally well, which does not provide readers with much useful information. On the other hand, relatively simplistic simulations, which only partially reflect real experimental data or which follow specific mathematical distributions, may be useful to evaluate a new method under certain assumptions; the simulation can then be adjusted to systematically interrogate the performance of the method. Tools are also available to assess the level of similarity between simulated datasets and experimental data according to certain characteristics or features of interest, or using visualizations \citep{Soneson2017}.

Experimental data, by definition, reflect the properties of real data. For benchmarking purposes, experimental datasets should be selected to reflect `typical' data for the given analysis task; e.g., containing commonly analyzed genes or cell populations. However, most experimental datasets do not contain a known ground truth, making it difficult or impossible to calculate quantitative performance metrics. Instead, methods may be evaluated by comparing them against each other, or against a widely accepted method or `gold standard' (e.g., comparing automated clustering methods against `manual gating' for defining cell populations in high-dimensional cytometry data \citep{Weber2016, Aghaeepour2013}). For some data types, it is also possible to design experimental datasets containing an approximate ground truth, which may be used as a proxy. Examples include: (i) including synthetic `spike-in' RNA molecules developed by the External RNA Control Consortium (ERCC) in RNA-sequencing experiments \citep{Jiang2011}; (ii) using genes located on the X chromosome as an approximate ground truth for silencing of genes or DNA methylation status (e.g., REF?); and (iii) using fluorescence-activated cell sorting (FACS) to sort cells into known subpopulations based on cell surface marker proteins prior to further analysis (e.g., REF?). Alternatively, experimental datasets may be evaluated qualitatively, for example by judging whether each method can recover a known biological result; however, this strategy will generally not enable a ranking of methods.

Another technique is to design `semi-simulated' benchmark datasets, which combine real experimental data with an `in silico' (i.e., computational) spike-in signal; e.g., by combining healthy cells from one sample with a subset of diseased cells from another sample. This strategy creates datasets containing both realistic experimental variability and a known ground truth, thus allowing performance metrics to be calculated (e.g., \citep{Weber2018}).




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Evaluation criteria: quantitative performance metrics}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The main performance evaluation and ranking should be based on a set of quantitative performance metrics. A wide variety of possible metrics exists. The choice of appropriate metrics will depend on the design and purpose of the benchmarking analysis, including the type of methods under consideration and the type of benchmark datasets used. Figure \ref{fig:performance_metrics}(a) provides an overview of commonly used performance metrics and the categories of analyses they are typically useful for.

For example, in a comparison of algorithms for a classification task such as clustering, with benchmark datasets containing a known ground truth or gold standard (e.g., \citep{Duo2018, Weber2016, Aghaeepour2013}), commonly used metrics to evaluate the performance of methods in recovering the known partition or set of clusters include the F1 score, adjusted Rand index, precision, and recall (sensitivity). These metrics may be calculated individually by class or cluster, or averaged (possibly weighted) across all classes. More generally, useful metrics for classification tasks include the true positive rate (TPR; sensitivity or recall), false positive rate (FPR; $1 -$ specificity), and false discovery rate (FDR). These types of metrics return a single value between 0 and 1 (or -1 and 1), which can be used to easily rank methods. In addition, metrics may be compared visually, e.g., using receiver operating characteristic (ROC) curves (TPR vs. FPR), TPR vs. FDR curves, or precision-recall (PR) curves. These visualizations can also be used as the basis to calculate further metrics, including area under the ROC curve (AUROC or AUC ROC) and area under the precision-recall curve (AUC PR). Figure \ref{fig:performance_metrics}(b) displays illustrative examples of several popular visualizations.

For other types of algorithms, different performance metrics will be appropriate. For algorithms that return continuous values, examples include the root mean square error (RMSE), distance measures, and correlation. For particularly complex or unusual data structures, custom metrics may need to be developed: for example, in our comparison of algorithms for single-cell trajectory inference \citep{Saelens2018b}, we developed custom metrics to evaluate the ability to reconstruct the network topology, as well as the ability to order cells along branches within the network. Such metrics should be validated across a wide range of possible predictions, as to avoid any bias in the evaluation. If multiple metrics are considered, the rankings according to the different metrics will need to be combined or weighted in some way, in order to produce a final ranking of methods.

Additional quantitative performance measures do not rely on a known ground truth or gold standard. These include measures of stability, stochasticity, robustness, and bias. For example, do methods give variable results when running multiple times? Are methods highly sensitive to input data? Do methods reliably run without errors? These measures may be quantified by running methods multiple times, e.g., using different `random seeds' (for methods that include a random start) or using subsampling to vary the input data. Finally, `null comparisons' can be used to investigate performance for simulated or experimental data that do not contain any true signal, providing information on expected error rates.


% Figure

\vskip 5mm

\begin{figure}[H]
\begin{center}
%\includegraphics[width=\textwidth]{.}
\end{center}
\caption{[2 main panels: top and bottom: (a) Flow chart / schematic; (b) Examples of visualizations of performance metrics.] (a) Schematic summarizing the main categories of quantitative performance metrics used for benchmarking analyses. (b) Examples of popular visualizations for comparing performance metrics for classification tasks, including (i) ROC curves, (ii) TPR-FDR curves (with points indicating FDR thresholds of 1\%, 5\%, and 10\%), (iii) precision-recall curves, and (iv) boxplots of F1 scores. Performance metric plots in (b)(i)--(iii) were generated with the iCOBRA R package \citep{Soneson2016}.}
\label{fig:performance_metrics}
\end{figure}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Evaluation criteria: other measures}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

While the main evaluation and ranking of methods should rely on the quantitative performance measures described above, methods should also be compared according to secondary measures, such as runtime, scalability, and other computational requirements; and qualitative aspects such as user-friendliness and documentation quality.

In our experience, runtimes and scalability can vary over several orders of magnitude between methods; e.g., in our comparison of clustering methods for high-dimensional cytometry data, runtimes ranged from minutes to days for the same analysis task on the same benchmark datasets \citep{Weber2016}. Similarly, other computational requirements such as memory or number of processors can vary widely. Depending on the scope of the benchmarking, runtimes and scalability may also be investigated systematically, e.g., by varying the number of cells in a single-cell RNA-sequencing dataset. In many cases, there is a tradeoff between performance and computational requirements, which should be clearly explained and discussed. For some users, if computational requirements for a top-performing method are prohibitive (e.g., requiring a high-performance server instead of a standard laptop), then a lower-ranked method may be preferred.

User-friendliness, installation procedures, code quality, and documentation quality can also be highly variable. These aspects are crucial for users without extensive computational training, which includes many experimental researchers. For example, methods that can be installed as packages from standard repositories or using package managers (such as CRAN or Bioconductor for R, or PyPI and pip for Python) are much more accessible than methods that are only available as custom scripts, or source code that must be compiled prior to use. Methods available as packages from GitHub or as pre-built binaries provide an intermediate level of accessibility. Availability across platforms (Windows, Mac, and Linux) and within commonly used programming languages for data analyses (R and Python) are also important. Availability of graphical user interfaces (GUIs) extends accessibility to users without any command-line experience; however, methods that are only available via GUIs can be difficult to evaluate during a systematic benchmarking, since scripts cannot be used.

In most cases, freely available and open source software will be preferred. Using freely available software ensures broad accessibility and makes it easier to interact with collaborators, while open source software can be checked for bugs or adapted by experienced end users or developers. From the developer perspective, code quality and use of software development best practices such as unit testing and continuous integration are also important. Similarly, adherence to commonly used data formats (e.g., GFF/GTF files for describing genomic features, BAM/SAM files for sequence alignment data, or FCS files for flow or mass cytometry data) greatly improves accessibility and extendability. Evidence of active package maintenance and recent bug fixes provide users with confidence that any unexpected problems are likely to be addressed.

For all end users, high-quality documentation is critical, including help pages for individual functions as well as extended tutorials or examples demonstrating how to run complete analysis pipelines. Ideally, documentation should be continually evaluated and checked; e.g., as done automatically by maintained repositories such as Bioconductor.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Parameters and software environment}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Parameter settings can have a crucial impact on method performance. For some methods, default parameters may give adequate performance, while other methods require significant effort to tune parameters to optimal values. Care should be taken to record all parameter settings used, to ensure that readers have complete information. For methods that are run using scripts, final parameter values will also be recorded within the scripts, which helps avoid errors. For methods only available via graphical interfaces, parameters must be recorded manually, for example in a spreadsheet. Reproducible workflow frameworks such as the Galaxy platform \citep{Afgan2018} are also helpful when using graphical tools. A summary table or spreadsheet of all parameter values should be provided along with the final publication describing the benchmarking; this may be included in supplementary information (e.g., Supporting Information Table S1 in \citep{Weber2016}).

Similarly, software versions can influence results, especially when method authors release major updates to methodology. Final results should be based on the latest available packages for each method, and software versions recorded. In the R programming language, the command `sessionInfo()` can be used to output a complete summary of all package versions loaded during an analysis, as well as the version of R and the operating system. Tools for creating reproducible and automated workflows, including Snakemake \citep{Koster2012}, Bioconda \citep{Gruning2018}, and workflowr \citep{Blischak2018} are also highly effective for keeping track of software versions and maintaining a consistent setup. Containerization tools such as Docker and Singularity may be used to completely encapsulate a software environment for each method, which preserves all versions used (including dependency packages and operating system), and facilitates the further distribution of methods to end users.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Interpretation, conclusions, and guidelines}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In order to make the results of a benchmarking analysis as useful as possible for readers, the performance metrics and other results must be clearly explained and interpreted. It is helpful to consider the interpretation from both the method user and developer perspective, and to summarize the results in the form of clear conclusions, guidelines, and recommendations. Depending on the context of the benchmarking, the interpretation should also involve biological or other domain knowledge to establish the scientific significance of the results.

For method users, the results of a benchmarking could be presented in the form of an overall ranking of methods, using either visualizations (e.g., Figure \ref{fig:performance_metrics}(b)) or summary tables (e.g., \citep{Aghaeepour2013, Weber2016}). If multiple key performance metrics were used, more complex visualizations are helpful; e.g., flow charts to guide the optimal choice of method for different types of data or analyses \citep{Saelens2018b}. Overall recommendations for the choice of method may also be provided. For method developers, the conclusions should also include guidelines for the future development of methods; e.g., identifying areas for improvement in the current state-of-the-art methods.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Publication and distribution of results}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Presenting the results and conclusions of a benchmarking in an accessible format ensures that readers will be able to find them and apply them to their own work. In academic research, the final results will usually be included in a peer-reviewed journal article. For an independent benchmarking, the benchmarking results will be the main focus of the paper. For a benchmarking designed to demonstrate the merits of a new method, the benchmarking results will form one part of the paper. We highly recommend publishing a preprint on a recognized preprint server prior to peer review; e.g., bioRxiv or arXiv. Publishing preprints greatly speeds up the distribution of results, and provides an opportunity to receive additional feedback prior to formal publication. Selecting an open access journal for the final publication also helps reduce barriers to access for readers.

For comprehensive benchmarkings, it is also useful to create online resources that enable readers to interactively explore the benchmarking results, or to highlight key results; e.g., \citep{Wiwie2015, Soneson2018}. For example, the Shiny framework for the R programming language allows results to be presented as an interactive HTML website (Figure \ref{fig:shiny}) [REF?]. Depending on computational expertise, interactive websites may also be created directly. Websites may be hosted on a private server or using commercial options such as \url{shinyapps.io}.


% Figure

\vskip 5mm

\begin{figure}[H]
\begin{center}
%\includegraphics[width=0.5\textwidth]{.}
\end{center}
\caption{[Screenshot of Shiny website.] Example of an interactive website for exploring the results of a benchmarking analysis [REF?], created using the Shiny framework for the R programming language.}
\label{fig:shiny}
\end{figure}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Enabling future extensions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In many fields of computational research, new methods are continually being developed \citep{Zappia2018}, which means that method benchmarkings can quickly become out of date. Benchmarking analyses should ideally be designed in an extensible way, so that new methods and new benchmark datasets can easily be added at a later date. This can be facilitated by creating publicly accessible code and data repositories, which can then be extended or adapted as necessary (e.g., \citep{Saelens2018a, Saelens2018b, Duo2018, Soneson2018, Weber2016}). It may also be useful to include preprocessed data objects if preprocessing steps for certain methods are highly computationally intensive. By making all code and data publicly accessible, extensions or adaptations may be added either by your own research group or by external groups, which benefits the research field as a whole and leads to wider readership. New results may be published as an online supplement, or added to an accompanying interactive website.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Reproducible research best practices}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Reproducibility of research findings has become an increasing concern \citep{Ioannidis2005}. In computational research, reproducibility of code scripts and data analyses represents a `minimum standard' that enables other researchers to verify analyses \citep{Peng2011}. Many free online tools are now available for these purposes, including GitHub for code sharing, publicly funded standard data repositories for certain data types, including ArrayExpress \citep{Kolesnikov2015}, the Gene Expression Omnibus \citep{Barrett2013}, and FlowRepository \citep{Spidlen2012}, and more general data repositories including Bioconductor's ExperimentHub, figshare, and Zenodo. Several repositories allow the creation of `digital object identifiers' (DOIs) for code or data objects, which can be used for citation purposes.

We recommend making all code and data (where permitted due to patient confidentiality) publicly available through these resources. In addition to ensuring a minimum standard of reproducibility, this also enables other computational researchers to build on and extend your benchmarking analyses, e.g., by `forking' and extending a code repository, or by re-using benchmark datasets. For example, our preprocessed benchmark datasets from our comparison of clustering methods for high-dimensional cytometry data \citep{Weber2016} have recently been re-used in several benchmarking analyses by independent research groups \citep{Konstorum2018, Abdelaal2018, Hu2018}. Similarly, our code repositories on GitHub have been `forked' and adapted by researchers working on other benchmarking efforts.

An extensive literature exists on best practices for computational research (e.g., \citep{Sandve2013}). For large-scale benchmarking studies, additional best practices from software development may also be useful, including the use of makefiles, unit testing, continuous integration, and containerization. While the optimal level of computational reproducibility will depend on the scope of the study and level of computational expertise, in our experience, almost all efforts in this area prove highly useful, especially during later revisions or extensions to the study.




\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Author contributions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

LMW proposed the project and drafted the manuscript. WS, RC, CS, YS, and MDR contributed ideas and contributed to drafting of the manuscript. YS and MDR supervised the project. All authors read and approved the final manuscript.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Competing interests}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The authors declare that they have no competing interests.




%%%%%%%%%%%%
% References
%%%%%%%%%%%%

\newpage


\singlespacing
\footnotesize

\bibliography{benchmarking_guidelines_paper}

\bibliographystyle{apa}




\end{document}



