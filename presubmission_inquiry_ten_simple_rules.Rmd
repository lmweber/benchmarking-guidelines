---
title: "Ten Simple Rules: Presubmission Inquiry"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output: html_document
---


PLOS Computational Biology\
Carlyle House\
Carlyle Road\
Cambridge, CB4 3DN\
United Kingdom



Zurich, 19 July 2018

**Presubmission Inquiry: 'Ten Simple Rules for a Method Benchmarking or Comparison'**


Dear Prof. Nussinov, dear Prof. Papin, dear Prof. Bourne, dear Editors,

We would like to submit a presubmission inquiry for a proposed article in the PLOS Computational Biology 'Ten Simple Rules' series, titled 'Ten Simple Rules for a Method Benchmarking or Comparison'.

We have found the Ten Simple Rules series to be a valuable resource for guidance on professional research practices. However, one topic that, in our view, is currently largely missing from the series is guidance on performing rigorous method benchmarkings or comparisons. Performance benchmarking is a critical aspect of computational method development, and our research groups have extensive experience on this topic (e.g., [1] or [2]). A 'Ten Simple Rules' article on this topic would allow us to communicate our procedures and best practices with the wider computational biology research community.

This article would be a joint submission from the Robinson lab at the University of Zurich, Switzerland, and the Saeys lab at VIB-UGent Center for Inflammation Research, Belgium.

The ten Rules we propose for this article are:

- Rule 1: What is the purpose of your benchmarking or comparison?
- Rule 2: Ensure that you have included all relevant methods.
- Rule 3: Select (or design) a variety of representative benchmark datasets.
- Rule 4: Evaluate methods according to statistical performance metrics.
- Rule 5: Compare runtimes, computational requirements, user-friendliness, and documentation.
- Rule 6: Record all parameter settings, as well as environment or session information.
- Rule 7: Consider your audience: interpret results and provide conclusions or guidelines from both user and method developer perspectives.
- Rule 8: Publish your results in an accessible format.
- Rule 9: Design the benchmarking to allow future extensions to include new methods or new datasets.
- Rule 10: Follow reproducible research best practices by making all code and data available in public repositories.

One previous article in the series ('Ten Simple Rules for Reducing Overoptimistic Reporting in Methodological Computational Research' [3]) partially covers some of these issues, but within a more specific context (namely, reducing bias due to poor design of method evaluations). Our proposed article would complement and extend this previous guidance by explaining our full set of procedures for method benchmarkings.

We believe that a full article based on the above Rules would provide a useful reference for computational biology researchers and method developers.

Thank you for your consideration, and we look forward to your reply.

Sincerely,



[1] Weber L. M. and Robinson M. D. (2016) *Comparison of clustering methods for high-dimensional single-cell flow and mass cytometry data.* Cytometry Part A, 89(12), 1084-1096: https://www.ncbi.nlm.nih.gov/pubmed/27992111

[2] Saelens W., Cannoodt R., Todorov H., and Saeys Y. (2018) *A comparison of single-cell trajectory inference methods: towards more accurate and robust tools.* BioRxiv preprint: https://www.biorxiv.org/content/early/2018/03/05/276907

[3] Boulesteix A.-L. (2015) *Ten Simple Rules for Reducing Overoptimistic Reporting in Methodological Computational Research.* PLOS Computational Biology, 11(4), e1004191: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4407963/



