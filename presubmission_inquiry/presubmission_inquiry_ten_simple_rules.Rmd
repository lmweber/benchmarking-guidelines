---
title: 'Ten Simple Rules: Presubmission Inquiry'
output:
  pdf_document: default
  html_document: default
date: "`r format(Sys.Date(), '%B %d, %Y')`"
---


PLOS Computational Biology\
Carlyle House\
Carlyle Road\
Cambridge, CB4 3DN\
United Kingdom



Zurich, 19 July 2018

**Presubmission Inquiry: Ten Simple Rules for Method Benchmarking**


Dear Prof. Nussinov, dear Prof. Papin, dear Prof. Bourne, dear Editors,

We would like to submit a presubmission inquiry for a proposed article in the PLOS Computational Biology 'Ten Simple Rules' series, titled 'Ten Simple Rules for Method Benchmarking'.  This article would be a joint submission from the Robinson lab at the University of Zurich, Switzerland, and the Saeys lab at VIB-UGent Center for Inflammation Research, Belgium.

We have found the Ten Simple Rules series to be an immensely valuable resource for guidance on a wide range of professional research practices. We believe that we can enhance this series by providing guidance on performing rigorous method benchmarking.  Such comparisons are a critical aspect of computational biology method development, and represent one theme of our research groups' activities (e.g., recent evaluations: [1], [2], [3], [4]). A 'Ten Simple Rules' article on this topic would allow us to communicate our experience in procedures and best practices with the wider computational biology community.

The ten Rules we propose for this article are:

- Rule 1: Define the purpose and scope of your benchmarking or comparison.
- Rule 2: Ensure that you have included all relevant methods.
- Rule 3: Select (or design) a variety of representative benchmark datasets.
- Rule 4: Evaluate methods according to statistical performance metrics.
- Rule 5: Compare runtimes, computational requirements, user-friendliness, and documentation.
- Rule 6: Record all parameter settings, as well as environment or session information.
- Rule 7: Consider your audience: interpret results and provide conclusions or guidelines from both user and method developer perspectives.
- Rule 8: Publish your results in an accessible format.
- Rule 9: Design the benchmarking to allow future extensions to include new methods or new datasets.
- Rule 10: Follow reproducible research best practices by making all code and data available in public repositories.

One previous article in the series ('Ten Simple Rules for Reducing Overoptimistic Reporting in Methodological Computational Research' [5]) partially covers some of these issues, but within a more specific context (namely, reducing bias due to poor design of method evaluations). Our proposed article would complement and extend this previous guidance by explaining our full set of procedures for method benchmarking.

We believe that a full article based on the above Rules would provide a useful reference for an audience of computational biology researchers and method developers.  The proposed author list (and order) is as follows: Lukas M. Weber, Wouter Saelens, Robrecht Cannoodt, Charlotte Soneson, Yvan Saeys, Mark D. Robinson.

Thank you for your consideration, and we look forward to your reply.

Sincerely,

Mark D. Robinson (on behalf of all authors),
Institute of Molecular Life Sciences,
University of Zurich


[1] Weber L. M. and Robinson M. D. (2016) *Comparison of clustering methods for high-dimensional single-cell flow and mass cytometry data.* Cytometry Part A, 89(12), 1084-1096: https://www.ncbi.nlm.nih.gov/pubmed/27992111

[2] Saelens W., Cannoodt R., Todorov H. and Saeys Y. (2018) *A comparison of single-cell trajectory inference methods: towards more accurate and robust tools.* BioRxiv preprint: https://www.biorxiv.org/content/early/2018/03/05/276907

[3] Soneson C. and Robinson M. D. (2018) *Bias, Robustness And Scalability In Differential Expression Analysis Of Single-Cell RNA-Seq Data.* Nature Methods 15, 255â€“261: https://www.ncbi.nlm.nih.gov/pubmed/29481549

[4] Saelens W., Cannoodt R. and Saeys Y. (2018) *A comprehensive evaluation of module detection methods for gene expression data* Nature Communications 9, 1090: https://www.ncbi.nlm.nih.gov/pubmed/29545622

[5] Boulesteix A.-L. (2015) *Ten Simple Rules for Reducing Overoptimistic Reporting in Methodological Computational Research.* PLOS Computational Biology, 11(4), e1004191: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4407963/
