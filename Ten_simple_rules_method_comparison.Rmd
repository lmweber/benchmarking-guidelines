---
title: "Ten Simple Rules"
author: "Lukas M. Weber, Charlotte Soneson, Mark D. Robinson"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Ten Simple Rules for a Method Benchmarking and Comparison

(or: Ten Simple Rules for a Method Evaluation and Comparison)

Written from the perspective of biological research, but most of the points below are also directly relevant in other fields.



## Rule 1: What is the purpose of your benchmarking or comparison?

Are you introducing a new method? Or performing an independent benchmarking of existing methods? (Why are you performing a method comparison? Do you need one?)

If introducing a new method: (i) thoroughly benchmark your new method, e.g. by demonstrating the effect of different options/arguments, and varying parameter settings; and (ii) benchmark against existing methods, using optimal parameter settings for each existing method (some parameter tuning for the existing methods will be required, but may not be particularly relevant to report all these results: main focus of your results should be on the merits of your new method; comparisons with existing methods should be streamlined e.g. using optimal parameter settings).

If performing an independent benchmarking: optimize parameter settings for all methods; report effects of key parameters for each method. (Is one method best overall? Or do some methods perform well in certain circumstances, with certain parameter settings?)
[MR: give an example - clustering, do we compare them all at the "true" number of clusters or each of them under their estimated number of clusters for a dataset]

Also: Ensure that you have found all relevant methods to compare against. (This may take some time to set up, e.g. different methods may be in different programming languages. See Rule X.)

[CS:] Do you evaluate entire "pipelines" (including preprocessing etc) or specific steps (e.g., clustering)?


## Rule 2: Select (or design) appropriate benchmark datasets.

Choose one or more benchmark datasets for your comparisons.  [MR: should we argue towards real datasets .. or keep it balanced between simulation/real; to discuss]

Two main options: simulated data and real (experimental) data. Simulated data allows you to calculate statistical performance metrics against a known "truth" (see Rule 3); while experimental data (by definition) may more accurately represent real data structure.

For some applications / data types, there may also be experimental data with known true signals, e.g. ERCC spike-ins for RNA-seq. [MR: give more examples here, such as using X chromosome for a (not complete but decent) truth for silencing of genes and/or DNA methylation status; Smith PNAS paper, voom paper]

Simulated and experimental can also be combined, e.g. computationally introducing an *in silico* spiked-in signal into experimental data (i.e. semi-simulated data).

Are publicly available datasets available? Or do you need to generate your own data (either simulated or experimental). Several public repositories available for certain types of data (see Rule Y).

[MR: just a basic recommendation: look at positive/negative controls] 



## Rule 3: If you are using simulated data, take care to design meaningful simulations.

Simulations should accurately reflect the features of real experimental data. This may be difficult to design, but is worth the effort.

[CS:] Not sure where to put this, but I think that sometimes there are reasons for having data not looking exactly like "real" data (e.g., smaller, exactly following a given distribution), at least when introducing a new method. You want to make sure that it works under the assumptions made by the model (maybe this is less benchmarking and more validation, though). [MR: highlight 'countsimQC' as one example of doing due diligence of comparing simulated to real datasets to capture as many aspects as possible]

If simulations are too simple, your benchmarking may not show anything meaningful: all methods may perform perfectly (or almost perfectly), which does not provide the reader with any useful information. Simulations should be designed in a way that the methods 'separate', i.e. the best methods perform well, while poor methods do not.

Use exploratory figures and hard thinking to decide whether your simulated data meaningfully simulate real experimental data.

Simulations are especially useful for interrogate methods in detail, i.e. what are the effects of varying certain parameter settings?

(More specific guidelines for how to design a good simulation?)



## Rule 4: Calculate statistical performance metrics.

Choose the most meaningful statistical performance metrics for your comparisons: examples include ROC curves, TPR-FDR curves, precision-recall curves, TPR, FPR. Also precision / recall / F1 scores, Adjusted Rand Index. Choice of metric depends on what exactly you are comparing.

[MR: somewhere make the argument that for discovery-based analyses, it's about getting a good tradeoff between sensitivity and error control, and that is why we often go towards TPR versus FDP]

Explain what some of these mean (what are true positives, false positives, FDR, etc.) Show formulas for F1 scores and Adjusted Rand Index.

(Include some example plots here: e.g. ROC curve, TPR-FDR curve, F1 scores.)

(Mention iCOBRA for generating plots.)

Note that these performance metrics all require a "truth" signal, i.e. simulated / semi-simulated data / experimental data with known true signals e.g. ERCC spike-ins. If you have used purely experimental data (which contains no truth signal), results will need to be interpreted qualitatively instead: has your method detected a particular (expected) signal of interest? (e.g. known signal of interest from previous studies).

[CS:] Many interesting evaluations can be made without truth: stability, biases, similarities between methods. Also null comparisons can often be generated without known truth.

Interpret the results from the statistical performance metrics: can you rank the methods? Does one method perform best overall? In many cases, different methods will perform best under different circumstances (e.g. different datasets); in this case, ensure that you have enough benchmark datasets to cover the main expected analysis types (e.g. for clustering single-cell data: detecting major populations vs. rare populations).

[CS:] As far as possible, use the same type of output from all methods. E.g., don't generate ROC curves from p-values from some methods and adjusted p-values from other methods. 



## Rule 5: Interpret results in biological terms.

The statistical performance metrics in Rule 4 all rely on a known "truth". However, results from your method should also make sense in biological terms. Analyze the results to determine whether they make sense in biological terms. (Visualizations are useful here: heatmaps, dimension reduction e.g. PCA, MDS, tSNE, UMAP.)

(Maybe include another example plot here: heatmap and dimensionality reduction; if relevant.)

This is mainly relevant for experimental data (with or without truth signal); for simulations this may also provide another good check that the simulated data is meaningful.

[CS:] This is maybe the most "controversial". What is "biological meaning" - can we always build a story that will favor the method we like? Also, the rest of the rules are valid more generally. 


## Rule 6: Record all parameter settings and session information.

Some methods may require a substantial amount of parameter tuning to optimize their performance. Record all final parameter settings used for the results presented in your comparison. This information can be included in supplementary information if you publish a paper. If the method can be run from the command line or using a script (or Galaxy?), save a script with the parameter settings (see Rule 10); if method is only available via graphical interface, take care to record all choices.

Also record the session information: e.g. operating system, programming language version (e.g. R or Python version), and package or library versions. Some methods may give different performance for different package versions or different operating systems, so this information is important for users.

In R, you can get a summary of session information with the command `sessionInfo()`.

Providing this information (parameters and session information) helps ensure reproducibility of your results (see Rule 10).

Could also use automated tools to preserve the setup used e.g. Docker; or workflow management tools e.g. `make` or `workflowr` (see Rule 10), [CS:] `conda`.

[CS:] Use up-to-date packages.



## Rule 7: Report runtimes and other computational requirements.

The main evaluations of the methods should rely on the statistical performance metrics (Rule 4): the most important thing is that a method performs well. However, in some cases, computational runtimes and memory requirements can vary over orders of magnitude for different methods that perform similar tasks (e.g. clustering methods in my clustering comparison paper). In this case, a method with much faster runtimes (or lower memory requirements) may be preferred over a method with slightly better performance (e.g. 1000x faster runtimes).

Computational requirements: can the method be run on a standard laptop, or is a high-performance server required? For some types of analysis, users can be expected to have access to a high-performance server (e.g. RNA-seq read alignment). However, for some other types of analysis (e.g. clustering flow/mass cytometry data), most readers may only have access to a standard laptop or desktop computer, so requirement a high-performance server is a limiting factor for the method (other methods with lower requirements will be available for the same task).

Consider installation procedures. Is the method easily available from a standard package library (e.g. CRAN or Bioconductor for R) or package manager (e.g. pip for Python)? Or is it available in an easy-to-install binary format (e.g. downloadable .pkg or .exe)? Or an easy-to-install package from GitHub? Or is a complicated installation procedure required, e.g. compiling from source? (if a complicated installation procedure is required, this may be a limiting factor for many users). Is the method being actively maintained (e.g. Bioconductor packages), or is it a package from GitHub with no new commits for 5 years?

[CS:] Also documentation (accessibility/completeness) can be important.

[MR: Is the software not only freely available, but open source.  In proteomics for example, there are some packages that are free, but closed source.  Also, re: Charlotte's comment and it came up with CvM, is the documentation and/or code evaluated - Wouter and Yvan did this in their recent trajectory preprint]

[MR: does the software use standard formats?  You can ask Simone about a piece of software we tried to use recently that used very specific format of a GFF/GTF file to get coordinates and relationships between transcripts and genes.  Was a real hassle]


## Rule 8: Decide on a strategy for presenting the results.

Publish as a standard academic paper in a journal? If so, consider also posting a preprint. Also consider open access (and bioRxiv preprint). When presenting a new method, the benchmarking will form part of the paperL i.e. showing that the method works well on the benchmark datasets, and comparing with the existing methods. Alternatively, if performing an independent benchmarking of existing methods, the benchmarking will be the main focus of the paper.

May also be useful to add an online resource (which can be referred to in the paper), e.g. an interactive website for exploring the benchmarking results. Can use R/Shiny to set up this type of resource (include link or screenshot of an example here?). May require web hosting on a private server or using commercial options such as `shinyapps.io`. Interactive HTML site for exploring benchmarking results.



## Rule 9: Maximize the usefulness of your benchmarking by making it extendable to new methods or new datasets.

New methods are always being developed, and a published benchmarking study may quickly become out-of-date, even by the time of publishing in a traditional journal.

If possible, set up your benchmarkings in a way that allows new methods or new benchmarking datasets to be added at a later date (e.g by adding new code scripts or datasets). Setting up publicly accessible repositories with all code scripts and datasets facilitates this. New results could be added as an online supplement on an accompanying website.

Future extensions may be added by yourself, or other readers in their own studies: making code and data public allows others to "fork" your repository and build on this for their own studies (see Rule 10).

[CS:] Provide also results, not just data. Otherwise anyone who wants to add a method must rerun all the other ones. ExperimentHub package?



## Rule 10: Make all code and data available in public repositories.

Reproducibility has become an increasing concern in biological research. Making all code and data files publicly available (where allowed due to confidentiality) greatly contributes to reproducibility. Many free repositories are now available for doing this, e.g. GitHub for code, and specialized data repositories for data, e.g. FlowRepository (flow/mass cytometry), ArrayExpress, Bioconductor ExperimentHub, figshare (more general).

Making all code and data publicly available allows other readers to extend and build on your comparisons in their own work. This should be seen as an opportunity, not a threat (you are not getting scooped; provides increased visibility and citations for your comparisons; seeing other people build on your work gives you ideas for own future work).

(Planning to make all code publicly available also helps to encourage yourself to follow better coding practices, e.g. writing more clearly documented code, which helps avoid errors.)

If you are publishing a paper, consider posting a preprint on bioRxiv: lets you circulate your results much earlier, provides improved visibility, often receive good feedback from readers, helps spot any errors before final publication.



