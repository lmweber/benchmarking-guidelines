Automatically generated by Mendeley Desktop 1.19.2
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Finak2018,
abstract = {A central tenet of reproducible research is that scientific results are published along with the underlying data and software code necessary to reproduce and verify the findings. A host of tools and software have been released that facilitate such work-flows and scientific journals have increasingly demanded that code and primary data be made available with publications. There has been little practical advice on implementing reproducible research work-flows for large 'omics' or systems biology data sets used by teams of analysts working in collaboration. In such instances it is important to ensure all analysts use the same version of a data set for their analyses. Yet, instantiating relational databases and standard operating procedures can be unwieldy, with high "startup" costs and poor adherence to procedures when they deviate substantially from an analyst's usual work-flow. Ideally a reproducible research work-flow should fit naturally into an individual's existing work-flow, with minimal disruption. Here, we provide an overview of how we have leveraged popular open source tools, including Bioconductor, Rmarkdown, git version control, R, and specifically R's package system combined with a new tool DataPackageR, to implement a lightweight reproducible research work-flow for preprocessing large data sets, suitable for sharing among small-to-medium sized teams of computational scientists. Our primary contribution is the DataPackageR tool, which decouples time-consuming data processing from data analysis while leaving a traceable record of how raw data is processed into analysis-ready data sets. The software ensures packaged data objects are properly documented and performs checksum verification of these along with basic package version management, and importantly, leaves a record of data processing code in the form of package vignettes. Our group has implemented this work-flow to manage, analyze and report on pre-clinical immunological trial data from multi-center, multi-assay studies for the past three years.},
author = {Finak, Greg and Mayer, Bryan and Fulp, William and Obrecht, Paul and Sato, Alicia and Chung, Eva and Holman, Drienna and Gottardo, Raphael},
doi = {10.12688/gatesopenres.12832.1},
file = {:Users/lukas/Dropbox/references/Mendeley/Finak2018{\_}DataPackageR.pdf:pdf},
issn = {2572-4754},
journal = {Gates Open Research},
number = {31},
title = {{DataPackageR: Reproducible data preprocessing, standardization and sharing using R/Bioconductor for collaborative data analysis}},
url = {https://gatesopenresearch.org/articles/2-31/v1},
volume = {2},
year = {2018}
}
@article{Boulesteix2015b,
abstract = {In computational sciences, including computational statistics, machine learning, and bioinformatics, it is often claimed in articles presenting new supervised learning methods that the new method performs better than existing methods on real data, for instance in terms of error rate. However, these claims are often not based on proper statistical tests and, even if such tests are performed, the tested hypothesis is not clearly defined and poor attention is devoted to the Type I and Type II errors. In the present article, we aim to fill this gap by providing a proper statistical framework for hypothesis tests that compare the performances of supervised learning methods based on several real datasets with unknown underlying distributions. After giving a statistical interpretation of ad hoc tests commonly performed by computational researchers, we devote special attention to power issues and outline a simple method of determining the number of datasets to be included in a comparison study to reach an adequat...},
author = {Boulesteix, Anne-Laure and Hable, Robert and Lauer, Sabine and Eugster, Manuel J. A.},
doi = {10.1080/00031305.2015.1005128},
file = {:Users/lukas/Dropbox/references/Mendeley/Boulesteix2015 A Statistical Framework for Hypothesis Testing in Real Data Comparison Studies.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
number = {3},
pages = {201--212},
title = {{A Statistical Framework for Hypothesis Testing in Real Data Comparison Studies}},
url = {http://www.tandfonline.com/doi/full/10.1080/00031305.2015.1005128},
volume = {69},
year = {2015}
}
@article{Nowicka2016,
author = {Nowicka, Malgorzata and Robinson, Mark D.},
doi = {10.12688/f1000research.8900.1},
file = {:Users/lukas/Dropbox/references/papers/Nowicka2016{\_}DRIMSeq.pdf:pdf},
issn = {2046-1402},
journal = {F1000Research},
pages = {1356},
pmid = {28105305},
title = {{DRIMSeq: a Dirichlet-multinomial framework for multivariate count outcomes in genomics}},
url = {http://f1000research.com/articles/5-1356/v1},
volume = {5},
year = {2016}
}
@article{Conchuir2015,
abstract = {The development and validation of computational macromolecular modeling and design methods depend on suitable benchmark datasets and informative metrics for comparing protocols. In addition, if a method is intended to be adopted broadly in diverse biological applications, there needs to be information on appropriate parameters for each protocol, as well as metrics describing the expected accuracy compared to experimental data. In certain disciplines, there exist established benchmarks and public resources where experts in a particular methodology are encouraged to supply their most efficient implementation of each particular benchmark. We aim to provide such a resource for protocols in macromolecular modeling and design. We present a freely accessible web resource (https://kortemmelab.ucsf.edu/benchmarks) to guide the development of protocols for protein modeling and design. The site provides benchmark datasets and metrics to compare the performance of a variety of modeling protocols using different computational sampling methods and energy functions, providing a "best practice" set of parameters for each method. Each benchmark has an associated downloadable benchmark capture archive containing the input files, analysis scripts, and tutorials for running the benchmark. The captures may be run with any suitable modeling method; we supply command lines for running the benchmarks using the Rosetta software suite. We have compiled initial benchmarks for the resource spanning three key areas: prediction of energetic effects of mutations, protein design, and protein structure prediction, each with associated state-of-the-art modeling protocols. With the help of the wider macromolecular modeling community, we hope to expand the variety of benchmarks included on the website and continue to evaluate new iterations of current methods as they become available.},
author = {Conch{\'{u}}ir, Shane O. and Barlow, Kyle A. and Pache, Roland A. and Ollikainen, Noah and Kundert, Kale and O'Meara, Matthew J. and Smith, Colin A. and Kortemme, Tanja},
doi = {10.1371/journal.pone.0130433},
file = {:Users/lukas/Dropbox/references/Mendeley/Conch{\'{u}}ir2015 A Web Resource for Standardized Benchmark Datasets.pdf:pdf},
isbn = {1932-6203},
issn = {19326203},
journal = {PLoS ONE},
number = {9},
pages = {e0130433},
pmid = {26335248},
title = {{A web resource for standardized benchmark datasets, metrics, and Rosetta protocols for macromolecular modeling and design}},
volume = {10},
year = {2015}
}
@article{MAQC2010,
abstract = {Gene expression data from microarrays are being applied to predict preclinical and clinical endpoints, but the reliability of these predictions has not been established. In the MAQC-II project, 36 independent teams analyzed six microarray data sets to generate predictive models for classifying a sample with respect to one of 13 endpoints indicative of lung or liver toxicity in rodents, or of breast cancer, multiple myeloma or neuroblastoma in humans. In total, {\textgreater}30,000 models were built using many combinations of analytical methods. The teams generated predictive models without knowing the biological meaning of some of the endpoints and, to mimic clinical reality, tested the models on data that had not been used for training. We found that model performance depended largely on the endpoint and team proficiency and that different approaches generated models of similar performance. The conclusions and recommendations from MAQC-II should be useful for regulatory agencies, study committees and independent investigators that evaluate methods for global gene expression analysis.},
author = {{MAQC Consortium}},
doi = {10.1038/nbt.1665},
file = {:Users/lukas/Dropbox/references/Mendeley/MAQCConsortium2010 The MicroArray Quality Control (MAQC)-II study.pdf:pdf},
isbn = {1546-1696 (Electronic)$\backslash$r1087-0156 (Linking)},
issn = {10870156},
journal = {Nature Biotechnology},
number = {8},
pages = {827--838},
pmid = {20676074},
title = {{The Microarray Quality Control (MAQC)-II study of common practices for the development and validation of microarray-based predictive models}},
volume = {28},
year = {2010}
}
@article{Costello2014,
abstract = {Predicting the best treatment strategy from genomic information is a core goal of precision medicine. Here we focus on predicting drug response based on a cohort of genomic, epigenomic and proteomic profiling data sets measured in human breast cancer cell lines. Through a collaborative effort between the National Cancer Institute (NCI) and the Dialogue on Reverse Engineering Assessment and Methods (DREAM) project, we analyzed a total of 44 drug sensitivity prediction algorithms. The top-performing approaches modeled nonlinear relationships and incorporated biological pathway information. We found that gene expression microarrays consistently provided the best predictive power of the individual profiling data sets; however, performance was increased by including multiple, independent data sets. We discuss the innovations underlying the top-performing methodology, Bayesian multitask MKL, and we provide detailed descriptions of all methods. This study establishes benchmarks for drug sensitivity prediction and identifies approaches that can be leveraged for the development of new methods.},
archivePrefix = {arXiv},
arxivId = {15334406},
author = {Costello, James C. and Heiser, Laura M. and Georgii, Elisabeth and G{\"{o}}nen, Mehmet and Menden, Michael P. and Wang, Nicholas J. and Bansal, Mukesh and Ammad-ud-din, Muhammad and Hintsanen, Petteri and Khan, Suleiman A. and Mpindi, John-Patrick and Kallioniemi, Olli and Honkela, Antti and Aittokallio, Tero and Wennerberg, Krister and {NCI DREAM Community} and Collins, James J. and Gallahan, Dan and Singer, Dinah and Saez-Rodriguez, Julio and Kaski, Samuel and Gray, Joe W. and Stolovitzky, Gustavo},
doi = {10.1038/nbt.2877},
eprint = {15334406},
file = {:Users/lukas/Dropbox/references/Mendeley/Costello2014 A community effort to assess and improve drug sensitivity prediction algorithms.pdf:pdf},
isbn = {1546-1696 (Electronic)$\backslash$r1087-0156 (Linking)},
issn = {15461696},
journal = {Nature Biotechnology},
number = {12},
pages = {1202--1212},
pmid = {24880487},
title = {{A community effort to assess and improve drug sensitivity prediction algorithms}},
volume = {32},
year = {2014}
}
@article{Dillies2012,
abstract = {During the last 3 years, a number of approaches for the normalization of RNA sequencing data have emerged in the literature, differing both in the type of bias adjustment and in the statistical strategy adopted. However, as data continue to accumulate, there has been no clear consensus on the appropriate normalization method to be used or the impact of a chosen method on the downstream analysis. In this work, we focus on a comprehensive comparison of seven recently proposed normalization methods for the differential analysis of RNA-seq data, with an emphasis on the use of varied real and simulated datasets involving different species and experimental designs to represent data characteristics commonly observed in practice. Based on this comparison study, we propose practical recommendations on the appropriate normalization method to be used and its impact on the differential analysis of RNA-seq data.},
author = {Dillies, Marie-Agn{\`{e}}s and Rau, Andrea and Aubert, Julie and Hennequet-Antier, Christelle and Jeanmougin, Marine and Servant, Nicolas and Keime, C{\'{e}}line and Marot, Guillemette and Castel, David and Estelle, Jordi and Guernec, Gregory and Jagla, Bernd and Jouneau, Luc and Lalo{\"{e}}, Denis and {Le Gall}, Caroline and Scha{\"{e}}ffer, Brigitte and {Le Crom}, St{\'{e}}phane and Guedj, Micka{\"{e}}l and Jaffr{\'{e}}zic, Florence},
doi = {10.1093/bib/bbs046},
file = {:Users/lukas/Dropbox/references/Mendeley/Dillies2012 A comprehensive evaluation of normalization methods.pdf:pdf},
isbn = {1477-4054 (Electronic)$\backslash$r1467-5463 (Linking)},
issn = {14675463},
journal = {Briefings in Bioinformatics},
keywords = {Differential analysis,High-throughput sequencing,Normalization,RNA-seq},
number = {6},
pages = {671--683},
pmid = {22988256},
title = {{A comprehensive evaluation of normalization methods for Illumina high-throughput RNA sequencing data analysis}},
volume = {14},
year = {2012}
}
@article{Hu2018b,
abstract = {Single-cell RNA sequencing (scRNA-seq) is a powerful tool to simultaneously sequencing the transcriptomes of a large number of individual cells at a high resolution. These data usually contain measurements of gene expression for many genes in thousands or tens of thousands of cells, though some datasets now reach the million-cell mark. Projecting high dimensional scRNA-seq data into a low dimensional space is essential for downstream analysis and data visualization. Many recent preprints accomplish this using variational autoencoders (VAE), generative models that learn underlying structure of data by compress it into a constrained, low dimensional space. The low dimensional spaces generated by VAEs have revealed complex patterns and novel biological signals from large-scale gene expression data and drug response predictions. Here, we evaluate a simple VAE approach for gene expression data, Tybalt, by training and measuring its performance on sets of simulated scRNA-seq data. We find a number of counter-intuitive performance changes: i.e., deeper neural networks can struggle when datasets contain more observations under some parameter configurations. We show that these methods are highly sensitive to parameter tuning: when tuned, the performance of the Tybalt model, which was not optimized for scRNA-seq data, outperforms other popular dimension reduction approaches - PCA, ZIFA, UMAP and t-SNE. On the other hand, without tuning performance can also be remarkably poor on the same data. Our results should discourage authors and reviewers from relying on empirical performance comparisons to evaluate the relative value of contributions in this area at this time. Instead, we recommend that attempts to compare or benchmark autoencoder methods for scRNA-seq data be performed by disinterested third parties or by methods developers only on unseen benchmark data that are provided to all participants simultaneously because the potential for performance differences due to unequal parameter tuning is so high.},
author = {Hu, Qiwen and Greene, Casey S},
doi = {10.1101/385534},
file = {:Users/lukas/Dropbox/references/Mendeley/Hu2018 Parameter tuning is a key part of dimensionality reduction via deep variational autoencoders.pdf:pdf},
journal = {bioRxiv},
title = {{Parameter tuning is a key part of dimensionality reduction via deep variational autoencoders for single cell RNA transcriptomics}},
url = {https://www.biorxiv.org/content/early/2018/08/05/385534},
year = {2018}
}
@article{Koster2012,
author = {K{\"{o}}ster, Johannes and Rahmann, Sven},
doi = {10.1093/bioinformatics/bts480},
file = {:Users/lukas/Dropbox/references/papers/K{\"{o}}ster2012{\_}snakemake.pdf:pdf},
journal = {Bioinformatics},
number = {19},
pages = {2520--2522},
title = {{Snakemake — a scalable bioinformatics workflow engine}},
volume = {28},
year = {2012}
}
@article{Aghaeepour2016,
abstract = {The Flow Cytometry: Critical Assessment of Population Identification Methods (FlowCAP) challenges were established to compare the performance of computational methods for identifying cell populations in multidimensional flow cytometry data. Here we report the results of FlowCAP-IV where algorithms from seven different research groups predicted the time to progression to AIDS among a cohort of 384 HIV+ subjects, using antigen-stimulated peripheral blood mononuclear cell (PBMC) samples analyzed with a 14-color staining panel. Two approaches (FlowReMi.1 and flowDensity-flowType-RchyOptimyx) provided statistically significant predictive value in the blinded test set. Manual validation of submitted results indicated that unbiased analysis of single cell phenotypes could reveal unexpected cell types that correlated with outcomes of interest in high dimensional flow cytometry datasets. {\textcopyright} 2015 International Society for Advancement of Cytometry},
author = {Aghaeepour, Nima and Chattopadhyay, Pratip and Chikina, Maria and Dhaene, Tom and {Van Gassen}, Sofie and Kursa, Miron and Lambrecht, Bart N and Malek, Mehrnoush and McLachlan, G J and Qian, Yu and Qiu, Peng and Saeys, Yvan and Stanton, Rick and Tong, Dong and Vens, Celine and Walkowiak, S{\l}awomir and Wang, Kui and Finak, Greg and Gottardo, Raphael and Mosmann, Tim and Nolan, Garry P and Scheuermann, Richard H and Brinkman, Ryan R},
doi = {10.1002/cyto.a.22732},
file = {:Users/lukas/Dropbox/references/papers/Aghaeepour2016{\_}FlowCAP{\_}IV.pdf:pdf},
issn = {15524930},
journal = {Cytometry Part A},
keywords = {Bioinformatics,Classification,Clinical outcome,Clustering,Data analysis,Flow cytometry,HIV,Supervised analysis},
pages = {16--21},
pmid = {26447924},
title = {{A Benchmark for Evaluation of Algorithms for Identification of Cellular Correlates of Clinical Outcomes}},
volume = {89A},
year = {2016}
}
@article{Arvaniti2017,
author = {Arvaniti, Eirini and Claassen, Manfred},
doi = {10.1038/ncomms14825},
file = {:Users/lukas/Dropbox/references/papers/Arvaniti2017 CellCnn.pdf:pdf},
journal = {Nature Communications},
number = {14825},
pages = {1--10},
publisher = {Nature Publishing Group},
title = {{Sensitive detection of rare disease-associated cell subsets via representation learning}},
url = {http://dx.doi.org/10.1038/ncomms14825},
volume = {8},
year = {2017}
}
@article{VanMechelen2018,
abstract = {To achieve scientific progress in terms of building a cumulative body of knowledge, careful attention to benchmarking is of the utmost importance. This means that proposals of new methods of data pre-processing, new data-analytic techniques, and new methods of output post-processing, should be extensively and carefully compared with existing alternatives, and that existing methods should be subjected to neutral comparison studies. To date, benchmarking and recommendations for benchmarking have been frequently seen in the context of supervised learning. Unfortunately, there has been a dearth of guidelines for benchmarking in an unsupervised setting, with the area of clustering as an important subdomain. To address this problem, discussion is given to the theoretical conceptual underpinnings of benchmarking in the field of cluster analysis by means of simulated as well as empirical data. Subsequently, the practicalities of how to address benchmarking questions in clustering are dealt with, and foundational recommendations are made.},
archivePrefix = {arXiv},
arxivId = {1809.10496},
author = {{Van Mechelen}, Iven and Boulesteix, Anne-Laure and Dangl, Rainer and Dean, Nema and Guyon, Isabelle and Hennig, Christian and Leisch, Friedrich and Steinley, Douglas},
eprint = {1809.10496},
file = {:Users/lukas/Dropbox/references/Mendeley/VanMechelen2018 Benchmarking in cluster analysis- A white paper.pdf:pdf},
journal = {arXiv},
keywords = {and phrases,benchmarking,cluster analysis,method evalu-},
title = {{Benchmarking in cluster analysis: A white paper}},
url = {https://arxiv.org/abs/1809.10496},
year = {2018}
}
@article{Afgan2018,
abstract = {High-throughput data production technologies, particularly ‘next-generation' DNA sequencing, have ushered in widespread and disruptive changes to biomedical research. Making sense of the large datasets produced by these technologies requires sophisticated statistical and computational methods, as well as substantial computational power. This has led to an acute crisis in life sciences, as researcherswithout informatics training attempt to perform computation-dependent analyses. Since 2005, the Galaxy project has worked to address this problem by providing a framework that makes advanced computational tools usable by non experts. Galaxy seeks to make data-intensive research more accessible, transparent and reproducible by providing a Web-based environment in which users can perform computational analyses and have all of the details automatically tracked for later inspection, publication, or reuse. In this report we highlight recently added features enabling biomedical analyses on a large scale.},
author = {Afgan, Enis and Baker, Dannon and Batut, B{\'{e}}r{\'{e}}nice and {Van Den Beek}, Marius and Bouvier, Dave and {\v{C}}ech, Martin and Chilton, John and Clements, Dave and Coraor, Nate and Gr{\"{u}}ning, Bj{\"{o}}rn A. and Guerler, Aysam and Hillman-Jackson, Jennifer and Hiltemann, Saskia and Jalili, Vahid and Rasche, Helena and Soranzo, Nicola and Goecks, Jeremy and Taylor, James and Nekrutenko, Anton and Blankenberg, Daniel},
doi = {10.1093/nar/gky379},
file = {:Users/lukas/Dropbox/references/papers/Afgan2018 The Galaxy platform for accessible, reproducible and collaborative biomedical analyses.pdf:pdf},
isbn = {13624962 (Electronic)},
issn = {13624962},
journal = {Nucleic Acids Research},
pages = {W537--W544},
pmid = {27137889},
title = {{The Galaxy platform for accessible, reproducible and collaborative biomedical analyses: 2018 update}},
volume = {46},
year = {2018}
}
@article{Soneson2016,
author = {Soneson, Charlotte and Robinson, Mark D},
doi = {10.1101/033431},
file = {:Users/lukas/Dropbox/references/papers/Soneson2016{\_}iCOBRA.pdf:pdf},
isbn = {1548-7105 (Electronic) 1548-7091 (Linking)},
issn = {1548-7091},
journal = {Nature Methods},
number = {4},
pages = {283},
pmid = {27027585},
publisher = {Nature Publishing Group},
title = {{iCOBRA: open, reproducible, standardized and live method benchmarking}},
url = {http://biorxiv.org/content/early/2015/12/01/033431.abstract},
volume = {13},
year = {2016}
}
@article{Hu2018a,
author = {Hu, Zicheng and Jujjavarapu, Chethan and Hughey, Jacob J and Andorf, Sandra and Lee, Hao-Chih and Gherardini, Pier Federico and Spitzer, Matthew H. and Thomas, Cristel G. and Campbell, John and Dunn, Patrick and Wiser, Jeff and Kidd, Brian A. and Dudley, Joel T. and Nolan, Garry P. and Bhattacharya, Sanchita and Butte, Atul J.},
doi = {10.1016/j.celrep.2018.07.003},
file = {:Users/lukas/Dropbox/references/papers/Hu2018{\_}MetaCyto.pdf:pdf},
issn = {2211-1247},
journal = {Cell Reports},
pages = {1377--1388},
publisher = {ElsevierCompany.},
title = {{MetaCyto: A Tool for Automated Meta-analysis of Mass and Flow Cytometry Data}},
url = {https://doi.org/10.1016/j.celrep.2018.07.003},
volume = {24},
year = {2018}
}
@article{Ioannidis2005,
abstract = {There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
archivePrefix = {arXiv},
arxivId = {gr-qc/0208024},
author = {Ioannidis, John P A},
doi = {10.1371/journal.pmed.0020124},
eprint = {0208024},
file = {:Users/lukas/Dropbox/references/Mendeley/Ioannidis2005 Why Most Published Research Findings Are False.pdf:pdf},
isbn = {3540239081},
issn = {15491277},
journal = {PLoS Medicine},
number = {8},
pages = {e124},
pmid = {16060722},
primaryClass = {gr-qc},
title = {{Why most published research findings are false}},
volume = {2},
year = {2005}
}
@article{Hill2016,
abstract = {It remains unclear whether causal, rather than merely correlational, relationships in molecular networks can be inferred in complex biological settings. Here we describe the HPN-DREAM network inference challenge, which focused on learning causal influences in signaling networks. We used phosphoprotein data from cancer cell lines as well as in silico data from a nonlinear dynamical model. Using the phosphoprotein data, we scored more than 2,000 networks submitted by challenge participants. The networks spanned 32 biological contexts and were scored in terms of causal validity with respect to unseen interventional data. A number of approaches were effective, and incorporating known biology was generally advantageous. Additional sub-challenges considered time-course prediction and visualization. Our results suggest that learning causal relationships may be feasible in complex settings such as disease states. Furthermore, our scoring approach provides a practical way to empirically assess inferred molecular networks in a causal sense.},
archivePrefix = {arXiv},
arxivId = {15334406},
author = {Hill, Steven M. and Heiser, Laura M. and Cokelaer, Thomas and Unger, Michael and Nesser, Nicole K. and Carlin, Daniel E. and Zhang, Yang and Sokolov, Artem and Paull, Evan O. and Wong, Chris K. and Graim, Kiley and Bivol, Adrian and Wang, Haizhou and Zhu, Fan and Afsari, Bahman and Danilova, Ludmila V. and Favorov, Alexander V. and Lee, Wai Shing and Taylor, Dane and Hu, Chenyue W. and Long, Byron L. and Noren, David P. and Bisberg, Alexander J. and {HPN-DREAM Consortium} and Mills, Gordon B. and Gray, Joe W. and Kellen, Michael and Norman, Thea and Friend, Stephen and Qutub, Amina A. and Fertig, Elana J. and Guan, Yuanfang and Song, Mingzhou and Stuart, Joshua M. and Spellman, Paul T. and Koeppl, Heinz and Stolovitzky, Gustavo and Saez-Rodriguez, Julio and Mukherjee, Sach},
doi = {10.1038/nmeth.3773},
eprint = {15334406},
file = {:Users/lukas/Dropbox/references/Mendeley/Hill2016 Inferring causal molecular networks.pdf:pdf},
isbn = {1548-7105 (Electronic)$\backslash$r1548-7091 (Linking)},
issn = {15487105},
journal = {Nature Methods},
number = {4},
pages = {310--322},
pmid = {26901648},
title = {{Inferring causal molecular networks: empirical assessment through a community-based effort}},
volume = {13},
year = {2016}
}
@misc{Wang2018,
author = {Wang, Gao and Stephens, Matthew and Carbonetto, Peter},
file = {:Users/lukas/Dropbox/references/Mendeley/Dynamic Statistical Comparisons.html:html},
title = {{DSC: Dynamic Statistical Comparisons}},
year = {2018}
}
@article{Barrett2013,
abstract = {The Gene Expression Omnibus (GEO, http://www.ncbi.nlm.nih.gov/geo/) is an international public repository for high-throughput microarray and next-generation sequence functional genomic data sets submitted by the research community. The resource supports archiving of raw data, processed data and metadata which are indexed, cross-linked and searchable. All data are freely available for download in a variety of formats. GEO also provides several web-based tools and strategies to assist users to query, analyse and visualize data. This article reports current status and recent database developments, including the release of GEO2R, an R-based web application that helps users analyse GEO data.},
author = {Barrett, Tanya and Wilhite, Stephen E. and Ledoux, Pierre and Evangelista, Carlos and Kim, Irene F. and Tomashevsky, Maxim and Marshall, Kimberly A. and Phillippy, Katherine H. and Sherman, Patti M. and Holko, Michelle and Yefanov, Andrey and Lee, Hyeseung and Zhang, Naigong and Robertson, Cynthia L. and Serova, Nadezhda and Davis, Sean and Soboleva, Alexandra},
doi = {10.1093/nar/gks1193},
file = {:Users/lukas/Dropbox/references/papers/Barrett2013 NCBI GEO - archive for functional genomics data sets - update.pdf:pdf},
isbn = {1362-4962 (Electronic)$\backslash$r0305-1048 (Linking)},
issn = {03051048},
journal = {Nucleic Acids Research},
pages = {D991--D995},
pmid = {23193258},
title = {{NCBI GEO: archive for functional genomics data sets - update}},
volume = {41},
year = {2013}
}
@article{Soneson2017,
abstract = {Background: As single-cell RNA-seq (scRNA-seq) is becoming increasingly common, the amount of publicly available data grows rapidly, generating a useful resource for computational method development and extension of published results. Although processed data matrices are typically made available in public repositories, the procedure to obtain these varies widely between data sets, which may complicate reuse and cross-data set comparison. Moreover, while many statistical methods for performing differential expression analysis of scRNA-seq data are becoming available, their relative merits and the performance compared to methods developed for bulk RNA-seq data are not sufficiently well understood. Results: We present conquer, a collection of consistently processed, analysis-ready public single-cell RNA-seq data sets. Each data set has count and transcripts per million (TPM) estimates for genes and transcripts, as well as quality control and exploratory analysis reports. We use a subset of the data sets available in conquer to perform an extensive evaluation of the performance and characteristics of statistical methods for differential gene expression analysis, evaluating a total of 30 statistical approaches on both experimental and simulated scRNA-seq data. Conclusions: Considerable differences are found between the methods in terms of the number and characteristics of the genes that are called differentially expressed. Pre-filtering of lowly expressed genes can have important effects on the results, particularly for some of the methods originally developed for analysis of bulk RNA-seq data. Generally, however, methods developed for bulk RNA-seq analysis do not perform notably worse than those developed specifically for scRNA-seq.},
author = {Soneson, Charlotte and Robinson, Mark D.},
doi = {10.1093/bioinformatics/btx631},
file = {:Users/lukas/Dropbox/references/papers/Soneson2017 Towards unified quality verification of synthetic count data with countsimQC.pdf:pdf},
issn = {14602059},
journal = {Bioinformatics},
number = {4},
pages = {691--692},
title = {{Towards unified quality verification of synthetic count data with countsimQC}},
volume = {34},
year = {2017}
}
@article{Kanitz2015,
abstract = {Understanding the regulation of gene expression, including transcription start site usage, alternative splicing, and polyadenylation, requires accurate quantification of expression levels down to the level of individual transcript isoforms. To comparatively evaluate the accuracy of the many methods that have been proposed for estimating transcript isoform abundance from RNA sequencing data, we have used both synthetic data as well as an independent experimental method for quantifying the abundance of transcript ends at the genome-wide level.},
author = {Kanitz, Alexander and Gypas, Foivos and Gruber, Andreas J. and Gruber, Andreas R. and Martin, Georges and Zavolan, Mihaela},
doi = {10.1186/s13059-015-0702-5},
file = {:Users/lukas/Dropbox/references/Mendeley/Kanitz2015 Comparative assessment of methods for the computational inference of transcript isoform abundance.pdf:pdf},
isbn = {10.1186/s13059-015-0702-5},
issn = {1474760X},
journal = {Genome Biology},
number = {150},
pmid = {26201343},
publisher = {Genome Biology},
title = {{Comparative assessment of methods for the computational inference of transcript isoform abundance from RNA-seq data}},
url = {http://dx.doi.org/10.1186/s13059-015-0702-5},
volume = {16},
year = {2015}
}
@article{TabulaMuris2018,
author = {{The Tabula Muris Consortium}},
doi = {10.1038/s41586-018-0590-4},
file = {:Users/lukas/Dropbox/references/Mendeley/TabulaMurisConsortium2018 Single-cell transcriptomics of 20 mouse organs creates a Tabula Muris.pdf:pdf},
issn = {0028-0836},
journal = {Nature},
pmid = {30283141},
title = {{Single-cell transcriptomics of 20 mouse organs creates a Tabula Muris}},
url = {http://www.nature.com/articles/s41586-018-0590-4},
year = {2018}
}
@article{Jiang2011,
abstract = {An international, peer-reviewed genome sciences journal featuring outstanding original research that offers novel insights into the biology of all organisms},
author = {Jiang, Lichun and Schlesinger, Felix and Davis, Carrie A. and Zhang, Yu and Li, Renhua and Salit, Marc and Gingeras, Thomas R. and Oliver, Brian},
doi = {10.1101/gr.121095.111},
file = {:Users/lukas/Dropbox/references/papers/Jiang2011 Synthetic spike-in standards for RNA-seq experiments.pdf:pdf},
isbn = {1549-5469 (Electronic)$\backslash$r1088-9051 (Linking)},
issn = {10889051},
journal = {Genome Research},
number = {9},
pages = {1543--1551},
pmid = {21816910},
title = {{Synthetic spike-in standards for RNA-seq experiments}},
volume = {21},
year = {2011}
}
@article{Weber2016,
abstract = {Recent technological developments in high-dimensional flow cytometry and mass cytometry (CyTOF) have made it possible to detect expression levels of dozens of protein markers in thousands of cells per second, allowing cell populations to be characterized in unprecedented detail. Traditional data analysis by "manual gating" can be inefficient and unreliable in these high-dimensional settings, which has led to the development of a large number of automated analysis methods. Methods designed for unsupervised analysis use specialized clustering algorithms to detect and define cell populations for further downstream analysis. Here, we have performed an up-to-date, extensible performance comparison of clustering methods for high-dimensional flow and mass cytometry data. We evaluated methods using several publicly available data sets from experiments in immunology, containing both major and rare cell populations, with cell population identities from expert manual gating as the reference standard. Several methods performed well, including FlowSOM, X-shift, PhenoGraph, Rclusterpp, and flowMeans. Among these, FlowSOM had extremely fast runtimes, making this method well-suited for interactive, exploratory analysis of large, high-dimensional data sets on a standard laptop or desktop computer. These results extend previously published comparisons by focusing on high-dimensional data and including new methods developed for CyTOF data. R scripts to reproduce all analyses are available from GitHub (https://github.com/lmweber/cytometry-clustering-comparison), and pre-processed data files are available from FlowRepository (FR-FCM-ZZPH), allowing our comparisons to be extended to include new clustering methods and reference data sets.},
author = {Weber, Lukas M and Robinson, Mark D},
doi = {10.1101/047613},
file = {:Users/lukas/Dropbox/references/papers/Weber2016{\_}Comparison{\_}of{\_}clustering{\_}methods.pdf:pdf},
issn = {15524922},
journal = {Cytometry Part A},
pages = {1084--1096},
pmid = {27992111},
title = {{Comparison of Clustering Methods for High-Dimensional Single-Cell Flow and Mass Cytometry Data}},
volume = {89A},
year = {2016}
}
@article{Fang2012,
abstract = {DNA methylation mediates imprinted gene expression by passing an epigenomic state across generations and differentially marking specific regulatory regions on maternal and paternal alleles. Imprinting has been tied to the evolution of the placenta in mammals and defects of imprinting have been associated with human diseases. Although recent advances in genome sequencing have revolutionized the study of DNA methylation, existing methylome data remain largely untapped in the study of imprinting. We present a statistical model to describe allele-specific methylation (ASM) in data from high-throughput short-read bisulfite sequencing. Simulation results indicate technical specifications of existing methylome data, such as read length and coverage, are sufficient for full-genome ASM profiling based on our model. We used our model to analyze methylomes for a diverse set of human cell types, including cultured and uncultured differentiated cells, embryonic stem cells and induced pluripotent stem cells. Regions of ASM identified most consistently across methylomes are tightly connected with known imprinted genes and precisely delineate the boundaries of several known imprinting control regions. Predicted regions of ASM common to multiple cell types frequently mark noncoding RNA promoters and represent promising starting points for targeted validation. More generally, our model provides the analytical complement to cutting-edge experimental technologies for surveying ASM in specific cell types and across species.},
author = {Fang, Fang and Hodges, Emily and Molaro, Antoine and Dean, Matthew and Hannon, Gregory J and Smith, Andrew D},
doi = {10.1073/pnas.1201310109},
file = {:Users/lukas/Dropbox/references/Mendeley/Fang2012 Genomic landscape of human allele-specific DNA methylation.pdf:pdf},
isbn = {1091-6490 (Electronic)$\backslash$r0027-8424 (Linking)},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences},
number = {19},
pages = {7332--7337},
pmid = {22523239},
title = {{Genomic landscape of human allele-specific DNA methylation}},
url = {http://www.pnas.org/cgi/doi/10.1073/pnas.1201310109},
volume = {109},
year = {2012}
}
@article{Wiwie2015,
abstract = {Identifying groups of similar objects is a popular first step in biomedical data analysis, but it is error-prone and impossible to perform manually. Many computational methods have been developed to tackle this problem. Here we assessed 13 well-known methods using 24 data sets ranging from gene expression to protein domains. Performance was judged on the basis of 13 common cluster validity indices. We developed a clustering analysis platform, ClustEval (http://clusteval.mpi-inf.mpg.de), to promote streamlined evaluation, comparison and reproducibility of clustering results in the future. This allowed us to objectively evaluate the performance of all tools on all data sets with up to 1,000 different parameter sets each, resulting in a total of more than 4 million calculated cluster validity indices. We observed that there was no universal best performer, but on the basis of this wide-ranging comparison we were able to develop a short guideline for biomedical clustering tasks. ClustEval allows biomedical researchers to pick the appropriate tool for their data type and allows method developers to compare their tool to the state of the art.},
author = {Wiwie, Christian and Baumbach, Jan and R{\"{o}}ttger, Richard},
doi = {10.1038/nmeth.3583},
file = {:Users/lukas/Dropbox/references/papers/Wiwie2015{\_}Comparing{\_}biomedical{\_}clustering{\_}methods.pdf:pdf},
isbn = {1548-7091},
issn = {1548-7105},
journal = {Nature Methods},
number = {11},
pages = {1033--1038},
pmid = {26389570},
title = {{Comparing the performance of biomedical clustering methods}},
url = {http://www.nature.com/doifinder/10.1038/nmeth.3583{\%}5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/26389570},
volume = {12},
year = {2015}
}
@misc{Blischak2018,
author = {Blischak, John and Carbonetto, Peter and Stephens, Matthew},
file = {:Users/lukas/Dropbox/references/Mendeley/Getting started with workflowr.html:html},
isbn = {9781457183300},
issn = {0076-6879},
pmid = {12073320},
title = {{workflowr: organized + reproducible + shareable data science in R}},
year = {2018}
}
@article{Zheng2017a,
abstract = {Benchmarking is an essential step in the development of computational tools. We take this opportunity to pitch in our opinions on tool benchmarking, in light of two correspondence articles published in Genome Biology.},
author = {Zheng, Siyuan},
doi = {10.1186/s13059-017-1258-3},
file = {:Users/lukas/Dropbox/references/Mendeley/Zheng2017 Benchmarking- contexts and details matter.pdf:pdf},
issn = {1474760X},
journal = {Genome Biology},
number = {129},
publisher = {Genome Biology},
title = {{Benchmarking: contexts and details matter}},
volume = {18},
year = {2017}
}
@article{Konstorum2018,
author = {Konstorum, Anna and Jekel, Nathan and Vidal, Emily and Laubenbacher, Reinhard},
doi = {10.1101/273862},
file = {:Users/lukas/Dropbox/references/Mendeley/Konstorum2018 Comparative Analysis of Linear and Nonlinear Dimension Reduction Techniques on Mass Cytometry Data.pdf:pdf},
journal = {bioRxiv},
title = {{Comparative Analysis of Linear and Nonlinear Dimension Reduction Techniques on Mass Cytometry Data}},
year = {2018}
}
@article{Saelens2018a,
abstract = {A critical step in the analysis of large genome-wide gene expression datasets is the use of module detection methods to group genes into co-expression modules. Because of limitations of classical clustering methods, numerous alternative module detection methods have been proposed, which improve upon clustering by handling co-expression in only a subset of samples, modelling the regulatory network, and/or allowing overlap between modules. In this study we use known regulatory networks to do a comprehensive and robust evaluation of these different methods. Overall, decomposition methods outperform all other strategies, while we do not find a clear advantage of biclustering and network inference-based approaches on large gene expression datasets. Using our evaluation workflow, we also investigate several practical aspects of module detection, such as parameter estimation and the use of alternative similarity measures, and conclude with recommendations for the further development of these methods.},
author = {Saelens, Wouter and Cannoodt, Robrecht and Saeys, Yvan},
doi = {10.1038/s41467-018-03424-4},
file = {:Users/lukas/Dropbox/references/papers/Saelens2018 A comprehensive evaluation of module detection methods.pdf:pdf},
isbn = {2041-1723 (Electronic) 2041-1723 (Linking)},
issn = {20411723},
journal = {Nature Communications},
pages = {1090},
pmid = {29545622},
title = {{A comprehensive evaluation of module detection methods for gene expression data}},
volume = {9},
year = {2018}
}
@article{Garalde2018,
abstract = {Ribonucleic acid sequencing can allow us to monitor the RNAs present in a sample. This enables us to detect the presence and nucleotide sequence of viruses, or to build a picture of how active transcriptional processes are changing -- information that is useful for understanding the status and function of a sample. Nanopore-based sequencing technology is capable of electronically analysing a sample's DNA directly, and in real-time. In this manuscript we demonstrate the ability of an array of nanopores to sequence RNA directly, and we apply it to a range of biological situations. Nanopore technology is the only available sequencing technology which can sequence RNA directly, rather than depending on reverse transcription and PCR. There are several potential advantages of this approach over other RNA-seq strategies, including the absence of amplification and reverse transcription biases, the ability to detect nucleotide analogues and the ability to generate full-length, strand-specific RNA sequences. This will improve the ease and speed of RNA analysis, while yielding richer biological information.},
author = {Garalde, Daniel R. and Snell, Elizabeth A. and Jachimowicz, Daniel and Sipos, Botond and Lloyd, Joseph H. and Bruce, Mark and Pantic, Nadia and Admassu, Tigist and James, Phillip and Warland, Anthony and Jordan, Michael and Ciccone, Jonah and Serra, Sabrina and Keenan, Jemma and Martin, Samuel and McNeill, Luke and Wallace, E. Jayne and Jayasinghe, Lakmal and Wright, Chris and Blasco, Javier and Young, Stephen and Brocklebank, Denise and Juul, Sissel and Clarke, James and Heron, Andrew J. and Turner, Daniel J.},
doi = {10.1038/nmeth.4577},
file = {:Users/lukas/Dropbox/references/Mendeley/Garalde2018 Highly parallel direct RNA sequencing on an array of nanopores.pdf:pdf},
isbn = {1548-7105},
issn = {15487105},
journal = {Nature Methods},
number = {3},
pages = {201--206},
pmid = {29334379},
publisher = {Nature Publishing Group},
title = {{Highly parallel direct RNA sequencing on an array of nanopores}},
url = {http://dx.doi.org/10.1038/nmeth.4577},
volume = {15},
year = {2018}
}
@article{Boulesteix2015a,
author = {Boulesteix, Anne-Laure},
doi = {10.1371/journal.pcbi.1004191},
file = {:Users/lukas/Dropbox/references/Mendeley/Boulesteix2015 Ten Simple Rules for Reducing Overoptimistic Reporting in Methodological Computational Research.pdf:pdf},
isbn = {1553-7358 (Electronic)$\backslash$r1553-734X (Linking)},
issn = {15537358},
journal = {PLoS Computational Biology},
number = {4},
pages = {e1004191},
pmid = {25905639},
title = {{Ten simple rules for reducing overoptimistic reporting in methodological computational research}},
volume = {11},
year = {2015}
}
@article{Aghaeepour2013,
abstract = {Traditional methods for flow cytometry (FCM) data processing rely on subjective manual gating. Recently, several groups have developed computational methods for identifying cell populations in multidimensional FCM data. The Flow Cytometry: Critical Assessment of Population Identification Methods (FlowCAP) challenges were established to compare the performance of these methods on two tasks: (i) mammalian cell population identification, to determine whether automated algorithms can reproduce expert manual gating and (ii) sample classification, to determine whether analysis pipelines can identify characteristics that correlate with external variables (such as clinical outcome). This analysis presents the results of the first FlowCAP challenges. Several methods performed well as compared to manual gating or external variables using statistical performance measures, which suggests that automated methods have reached a sufficient level of maturity and accuracy for reliable use in FCM data analysis. {\textcopyright} 2013 Nature America, Inc. All rights reserved.},
author = {Aghaeepour, Nima and Finak, Greg and {The FlowCAP Consortium} and {The DREAM Consortium} and Hoos, Holger and Mosmann, Tim R and Brinkman, Ryan and Gottardo, Raphael and Scheuermann, Richard H},
doi = {10.1038/NMETH.2365},
file = {:Users/lukas/Dropbox/references/papers/Aghaeepour2013{\_}FlowCAP{\_}I{\_}and{\_}II.pdf:pdf},
isbn = {1548-7105 (Electronic)$\backslash$r1548-7091 (Linking)},
issn = {15487091 15487105},
journal = {Nature Methods},
number = {3},
pages = {228--238},
pmid = {23396282},
title = {{Critical assessment of automated flow cytometry data analysis techniques}},
volume = {10},
year = {2013}
}
@article{Saelens2018b,
author = {Saelens, Wouter and Cannoodt, Robrecht and Todorov, Helena and Saeys, Yvan},
doi = {10.1101/276907},
file = {:Users/lukas/Dropbox/references/papers/Saelens2018 A comparison of single-cell trajectory inference methods.pdf:pdf},
journal = {bioRxiv},
title = {{A comparison of single-cell trajectory inference methods: towards more accurate and robust tools}},
year = {2018}
}
@article{Korthauer2018,
abstract = {With recent advances in sequencing technology, it is now feasible to measure DNA methylation at tens of millions of sites across the entire genome. In most applications, biologists are interested in detecting differentially methylated regions, composed of multiple sites with differing methylation levels among populations. However, current computational approaches for detecting such regions do not provide accurate statistical inference. A major challenge in reporting uncertainty is that a genome-wide scan is involved in detecting these regions, which needs to be accounted for. A further challenge is that sample sizes are limited due to the costs associated with the technology. We have developed a new approach that overcomes these challenges and assesses uncertainty for differentially methylated regions in a rigorous manner. Region-level statistics are obtained by fitting a generalized least squares (GLS) regression model with a nested autoregressive correlated error structure for the effect of interest on transformed methylation proportions. We develop an inferential approach, based on a pooled null distribution, that can be implemented even when as few as two samples per population are available. Here we demonstrate the advantages of our method using both experimental data and Monte Carlo simulation. We find that the new method improves the specificity and sensitivity of list of regions and accurately controls the False Discovery Rate (FDR).},
author = {Korthauer, Keegan and Chakraborty, Sutirtha and Benjamini, Yuval and Irizarry, Rafael A},
doi = {10.1093/biostatistics/kxy007},
file = {:Users/lukas/Dropbox/references/Mendeley/Korthauer2018 Detection and accurate false discovery rate control of differentially methylated regions.pdf:pdf},
issn = {1465-4644},
journal = {Biostatistics},
keywords = {bisulfite sequencing,differential methylation,false discovery rate,generalized least squares},
pages = {1--17},
pmid = {29481604},
title = {{Detection and accurate false discovery rate control of differentially methylated regions from whole genome bisulfite sequencing}},
url = {https://academic.oup.com/biostatistics/advance-article/doi/10.1093/biostatistics/kxy007/4899074},
year = {2018}
}
@article{Zhou2014,
abstract = {A popular approach for comparing gene expression levels between (replicated) conditions of RNA sequencing data relies on counting reads that map to features of interest. Within such count-based methods, many flexible and advanced statistical approaches now exist and offer the ability to adjust for covariates (e.g. batch effects). Often, these methods include some sort of 'sharing of information' across features to improve inferences in small samples. It is important to achieve an appropriate tradeoff between statistical power and protection against outliers. Here, we study the robustness of existing approaches for count-based differential expression analysis and propose a new strategy based on observation weights that can be used within existing frameworks. The results suggest that outliers can have a global effect on differential analyses. We demonstrate the effectiveness of our new approach with real data and simulated data that reflects properties of real datasets (e.g. dispersion-mean trend) and develop an extensible framework for comprehensive testing of current and future methods. In addition, we explore the origin of such outliers, in some cases highlighting additional biological or technical factors within the experiment. Further details can be downloaded from the project website: http://imlspenticton.uzh.ch/robinson{\_}lab/edgeR{\_}robust/.},
archivePrefix = {arXiv},
arxivId = {1312.3382},
author = {Zhou, Xiaobei and Lindsay, Helen and Robinson, Mark D.},
doi = {10.1093/nar/gku310},
eprint = {1312.3382},
file = {:Users/lukas/Dropbox/references/papers/Zhou2014{\_}Robustly{\_}detecting{\_}differential{\_}expression.pdf:pdf},
isbn = {1362-4962 (Electronic)$\backslash$r0305-1048 (Linking)},
issn = {13624962},
journal = {Nucleic Acids Research},
number = {11},
pages = {e91},
pmid = {24753412},
title = {{Robustly detecting differential expression in RNA sequencing data using observation weights}},
volume = {42},
year = {2014}
}
@article{Eduati2015,
abstract = {The ability to computationally predict the effects of toxic compounds on humans could help address the deficiencies of current chemical safety testing. Here, we report the results from a community-based DREAM challenge to predict toxicities of environmental compounds with potential adverse health effects for human populations. We measured the cytotoxicity of 156 compounds in 884 lymphoblastoid cell lines for which genotype and transcriptional data are available as part of the Tox21 1000 Genomes Project. The challenge participants developed algorithms to predict interindividual variability of toxic response from genomic profiles and population-level cytotoxicity data from structural attributes of the compounds. 179 submitted predictions were evaluated against an experimental data set to which participants were blinded. Individual cytotoxicity predictions were better than random, with modest correlations (Pearson's r {\textless} 0.28), consistent with complex trait genomic prediction. In contrast, predictions of population-level response to different compounds were higher (r {\textless} 0.66). The results highlight the possibility of predicting health risks associated with unknown compounds, although risk estimation accuracy remains suboptimal.},
author = {Eduati, Federica and Mangravite, Lara M. and Wang, Tao and Tang, Hao and Bare, J. Christopher and Huang, Ruili and Norman, Thea and Kellen, Mike and Menden, Michael P. and Yang, Jichen and Zhan, Xiaowei and Zhong, Rui and Xiao, Guanghua and Xia, Menghang and Abdo, Nour and Kosyk, Oksana and Collaboration, the NIEHS-NCATS-UNC DREAM Toxicogenetics and Friend, Stephen and Dearry, Allen and Simeonov, Anton and Tice, Raymond R. and Rusyn, Ivan and Wright, Fred A. and Stolovitzky, Gustavo and Xie, Yang and Saez-Rodriguez, Julio},
doi = {10.1038/nbt.3299},
file = {:Users/lukas/Dropbox/references/papers/Eduati2015 Prediction of human population responses to toxic compounds by a collaborative competition.pdf:pdf},
isbn = {doi:10.1038/nbt.3299},
issn = {15461696},
journal = {Nature Biotechnology},
number = {9},
pages = {933--940},
pmid = {26258538},
title = {{Prediction of human population responses to toxic compounds by a collaborative competition}},
volume = {33},
year = {2015}
}
@article{Sage2015,
abstract = {The quality of super-resolution images obtained by single-molecule localization microscopy (SMLM) depends largely on the software used to detect and accurately localize point sources. In this work, we focus on the computational aspects of super-resolution microscopy and present a comprehensive evaluation of localization software packages. Our philosophy is to evaluate each package as a whole, thus maintaining the integrity of the software. We prepared synthetic data that represent three-dimensional structures modeled after biological components, taking excitation parameters, noise sources, point-spread functions and pixelation into account. We then asked developers to run their software on our data; most responded favorably, allowing us to present a broad picture of the methods available. We evaluated their results using quantitative and user-interpretable criteria: detection rate, accuracy, quality of image reconstruction, resolution, software usability and computational resources. These metrics reflect the various tradeoffs of SMLM software packages and help users to choose the software that fits their needs.},
author = {Sage, Daniel and Kirshner, Hagai and Pengo, Thomas and Stuurman, Nico and Min, Junhong and Manley, Suliana and Unser, Michael},
doi = {10.1038/nmeth.3442},
file = {:Users/lukas/Dropbox/references/Mendeley/Sage2015 Quantitative evaluation of software packages for single-molecule localization microscopy.pdf:pdf},
isbn = {doi:10.1038/nmeth.3442},
issn = {15487105},
journal = {Nature Methods},
number = {8},
pages = {717--724},
pmid = {26076424},
title = {{Quantitative evaluation of software packages for single-molecule localization microscopy}},
volume = {12},
year = {2015}
}
@article{Duo2018,
abstract = {Subpopulation identification, usually via some form of unsupervised clustering, is a fundamental step in the analysis of many single-cell RNA-seq data sets. This has motivated the development and application of a broad range of clustering methods, based on various underlying algorithms. Here, we provide a systematic and extensible performance evaluation of 12 clustering algorithms, including both methods developed explicitly for scRNA-seq data and more general-purpose methods. The methods were evaluated using 9 publicly available scRNA-seq data sets as well as three simulations with varying degree of cluster separability. The same feature selection approaches were used for all methods, allowing us to focus on the investigation of the performance of the clustering algorithms themselves. We evaluated the ability of recovering known subpopulations, the stability and the run time of the methods. Additionally, we investigated whether the performance could be improved by generating consensus partitions from multiple individual clustering methods. We found substantial differences in the performance, run time and stability between the methods, with SC3 and Seurat showing the most favorable results. Additionally, we found that consensus clustering typically did not improve the performance compared to the best of the combined methods, but that several of the top-performing methods already perform some type of consensus clustering. The R scripts providing an extensible framework for the evaluation of new methods and data sets are available on GitHub ( https://github.com/markrobinsonuzh/scRNAseq{\_}clustering{\_}comparison ).},
author = {Du{\`{o}}, Angelo and Robinson, Mark D. and Soneson, Charlotte},
doi = {10.12688/f1000research.15666.1},
file = {:Users/lukas/Dropbox/references/papers/Duo2018 A systematic performance evaluation of clustering methods.pdf:pdf},
issn = {2046-1402},
journal = {F1000Research},
keywords = {benchmarking,clustering,clustering methods,rna-seq,single-cell rna-seq},
pages = {1141},
title = {{A systematic performance evaluation of clustering methods for single-cell RNA-seq data}},
url = {https://f1000research.com/articles/7-1141/v1},
volume = {7},
year = {2018}
}
@article{Law2014,
abstract = {New normal linear modeling strategies are presented for analyzing read counts from RNA-seq experiments. The voom method estimates the mean-variance relationship of the log-counts, generates a precision weight for each observation and enters these into the limma empirical Bayes analysis pipeline. This opens access for RNA-seq analysts to a large body of methodology developed for microarrays. Simulation studies show that voom performs as well or better than count-based RNA-seq methods even when the data are generated according to the assumptions of the earlier methods. Two case studies illustrate the use of linear modeling and gene set testing methods.},
author = {Law, Charity W. and Chen, Yunshun and Shi, Wei and Smyth, Gordon K.},
doi = {10.1186/gb-2014-15-2-r29},
file = {:Users/lukas/Dropbox/references/papers/Law2014{\_}Voom.pdf:pdf},
isbn = {1465-6906},
issn = {1474-760X},
journal = {Genome Biology},
pages = {R29},
pmid = {24485249},
title = {{voom: precision weights unlock linear model analysis tools for RNA-seq read counts}},
url = {http://dx.doi.org/10.1186/gb-2014-15-2-r29{\%}5Cnhttp://genomebiology.biomedcentral.com/articles/10.1186/gb-2014-15-2-r29{\%}5Cnhttp://genomebiology.biomedcentral.com/track/pdf/10.1186/gb-2014-15-2-r29?site=genomebiology.biomedcentral.com},
volume = {15},
year = {2014}
}
@article{Rapaport2013,
abstract = {A large number of computational methods have been developed for analyzing differential gene expression in RNA-seq data. We describe a comprehensive evaluation of common methods using the SEQC benchmark dataset and ENCODE data. We consider a number of key features, including normalization, accuracy of differential expression detection and differential expression analysis when one condition has no detectable expression. We find significant differences among the methods, but note that array-based methods adapted to RNA-seq data perform comparably to methods designed for RNA-seq. Our results demonstrate that increasing the number of replicate samples significantly improves detection power over increased sequencing depth.},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.5277v2},
author = {Rapaport, Franck and Khanin, Raya and Liang, Yupu and Pirun, Mono and Krek, Azra and Zumbo, Paul and Mason, Christopher E and Socci, Nicholas D and Betel, Doron},
doi = {10.1186/gb-2013-14-9-r95},
eprint = {arXiv:1301.5277v2},
file = {:Users/lukas/Dropbox/references/Mendeley/Rapaport2013 Comprehensive evaluation of differential gene expression analysis methods.pdf:pdf},
isbn = {1465-6906},
issn = {1465-6914},
journal = {Genome Biology},
number = {R95},
pmid = {24020486},
title = {{Comprehensive evaluation of differential gene expression analysis methods for RNA-seq data}},
url = {http://dx.doi.org/10.1186/gb-2013-14-9-r95},
volume = {14},
year = {2013}
}
@article{Rigaill2018,
abstract = {Numerous statistical pipelines are now available for the differential analysis of gene expression measured with RNA-sequencing technology. Most of them are based on similar statistical frameworks after normalization, differing primarily in the choice of data distribution, mean and variance estimation strategy and data filtering. We propose an evaluation of the impact of these choices when few biological replicates are available through the use of synthetic data sets. This framework is based on real data sets and allows the exploration of various scenarios differing in the proportion of non-differentially expressed genes. Hence, it provides an evaluation of the key ingredients of the differential analysis, free of the biases associated with the simulation of data using parametric models. Our results show the relevance of a proper modeling of the mean by using linear or generalized linear modeling. Once the mean is properly modeled, the impact of the other parameters on the performance of the test is much less important. Finally, we propose to use the simple visualization of the raw P-value histogram as a practical evaluation criterion of the performance of differential analysis methods on real data sets.},
author = {Rigaill, Guillem and Balzergue, Sandrine and Brunaud, V{\'{e}}ronique and Blondet, Eddy and Rau, Andrea and Rogier, Odile and Caius, Jos{\'{e}} and Maugis-Rabusseau, Cathy and Soubigou-Taconnat, Ludivine and Aubourg, S{\'{e}}bastien and Lurin, Claire and Martin-Magniette, Marie-Laure and Delannoy, Etienne},
doi = {10.1093/bib/bbw092},
file = {:Users/lukas/Dropbox/references/Mendeley/Rigaill2018 Synthetic data sets for the identification of key ingredients for RNA-seq differential analysis.pdf:pdf},
issn = {14774054},
journal = {Briefings in Bioinformatics},
keywords = {Benchmark data set,Differential analysis,RNA-seq},
number = {1},
pages = {65--76},
pmid = {27742662},
title = {{Synthetic data sets for the identification of key ingredients for RNA-seq differential analysis}},
volume = {19},
year = {2018}
}
@article{SEQCMAQCIII2014,
abstract = {We present primary results from the Sequencing Quality Control (SEQC) project, coordinated by the US Food and Drug Administration. Examining Illumina HiSeq, Life Technologies SOLiD and Roche 454 platforms at multiple laboratory sites using reference RNA samples with built-in controls, we assess RNA sequencing (RNA-seq) performance for junction discovery and differential expression profiling and compare it to microarray and quantitative PCR (qPCR) data using complementary metrics. At all sequencing depths, we discover unannotated exon-exon junctions, with {\textgreater}80{\%} validated by qPCR. We find that measurements of relative expression are accurate and reproducible across sites and platforms if specific filters are used. In contrast, RNA-seq and microarrays do not provide accurate absolute measurements, and gene-specific biases are observed for all examined platforms, including qPCR. Measurement performance depends on the platform and data analysis pipeline, and variation is large for transcript-level profiling. The complete SEQC data sets, comprising {\textgreater}100 billion reads (10Tb), provide unique resources for evaluating RNA-seq analyses for clinical and regulatory settings.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {{SEQC/MAQC-III Consortium}},
doi = {10.1038/nbt.2957},
eprint = {NIHMS150003},
file = {:Users/lukas/Dropbox/references/Mendeley/SEQC-MAQC-III Consortium2014 A comprehensive assessment of RNA-seq accuracy.pdf:pdf},
isbn = {1087-0156$\backslash$r1546-1696},
issn = {15461696},
journal = {Nature Biotechnology},
number = {9},
pages = {903--914},
pmid = {25150838},
title = {{A comprehensive assessment of RNA-seq accuracy, reproducibility and information content by the Sequencing Quality Control Consortium}},
volume = {32},
year = {2014}
}
@article{Gruning2018,
author = {Gr{\"{u}}ning, Bj{\"{o}}rn and Dale, Ryan and Sj{\"{o}}din, Andreas and Chapman, Brad A. and Rowe, Jillian and Tomkins-Tinch, Christopher H. and Valieris, Renan and K{\"{o}}ster, Johannes and {The Bioconda Team}},
doi = {10.1038/s41592-018-0046-7},
file = {:Users/lukas/Dropbox/references/papers/Gr{\"{u}}ning2018{\_}Bioconda.pdf:pdf},
journal = {Nature Methods},
title = {{Bioconda : sustainable and comprehensive software distribution for the life sciences}},
volume = {July},
year = {2018}
}
@article{Boulesteix2013,
abstract = {In computational science literature including, e.g., bioinformatics, computational statistics or machine learning, most published articles are devoted to the development of "new methods", while comparison studies are generally appreciated by readers but surprisingly given poor consideration by many journals. This paper stresses the importance of neutral comparison studies for the objective evaluation of existing methods and the establishment of standards by drawing parallels with clinical research. The goal of the paper is twofold. Firstly, we present a survey of recent computational papers on supervised classification published in seven high-ranking computational science journals. The aim is to provide an up-to-date picture of current scientific practice with respect to the comparison of methods in both articles presenting new methods and articles focusing on the comparison study itself. Secondly, based on the results of our survey we critically discuss the necessity, impact and limitations of neutral comparison studies in computational sciences. We define three reasonable criteria a comparison study has to fulfill in order to be considered as neutral, and explicate general considerations on the individual components of a "tidy neutral comparison study". R codes for completely replicating our statistical analyses and figures are available from the companion website http://www.ibe.med.uni-muenchen.de/organisation/mitarbeiter/020{\_}professuren/boulesteix/plea2013.},
archivePrefix = {arXiv},
arxivId = {arXiv:1208.2651v1},
author = {Boulesteix, Anne-Laure and Lauer, Sabine and Eugster, Manuel J.A.},
doi = {10.1371/journal.pone.0061562},
eprint = {arXiv:1208.2651v1},
file = {:Users/lukas/Dropbox/references/Mendeley/Boulesteix2013 A Plea for Neutral Comparison Studies in Computational Sciences.pdf:pdf},
issn = {19326203},
journal = {PLoS ONE},
number = {4},
pages = {e61562},
pmid = {23637855},
title = {{A Plea for Neutral Comparison Studies in Computational Sciences}},
volume = {8},
year = {2013}
}
@article{Kimes2018,
abstract = {Benchmark studies are widely used to compare and evaluate tools developed for answering various biological questions. Despite the popularity of these comparisons, the implementation is often ad hoc, with little consistency across studies. To address this problem, we developed SummarizedBenchmark, an R package and framework for organizing and structuring benchmark comparisons. SummarizedBenchmark defines a general grammar for benchmarking and allows for easier setup and execution of benchmark comparisons, while improving the reproducibility and replicability of such comparisons. We demonstrate the wide applicability of our framework using four examples from different applications.},
author = {Kimes, Patrick K and Reyes, Alejandro},
doi = {10.1093/bioinformatics/bty627},
file = {:Users/lukas/Dropbox/references/Mendeley/Kimes2018{\_}SummarizedBenchmark.pdf:pdf},
issn = {1367-4803},
journal = {Bioinformatics},
pages = {1--3},
title = {{Reproducible and replicable comparisons using SummarizedBenchmark}},
url = {https://academic.oup.com/bioinformatics/advance-article/doi/10.1093/bioinformatics/bty627/5055124},
year = {2018}
}
@article{Soneson2018,
author = {Soneson, Charlotte and Robinson, Mark D},
doi = {10.1038/nmeth.4612},
file = {:Users/lukas/Dropbox/references/papers/Soneson2018 Bias, robustness and scalability in single-cell differential.pdf:pdf},
issn = {1548-7091},
journal = {Nature Methods},
number = {4},
pages = {255--261},
pmid = {29481549},
publisher = {Nature Publishing Group},
title = {{Bias, robustness and scalability in single-cell differential expression analysis}},
url = {http://www.nature.com/doifinder/10.1038/nmeth.4612},
volume = {15},
year = {2018}
}
@article{Bokulich2016,
author = {Bokulich, Nicholas A and Rideout, Jai Ram and Mercurio, William G and Shiffer, Arron and Wolfe, Benjamin and Maurice, Corinne F and Dutton, Rachel J and Turnbaugh, Peter J and Knight, Rob and Caporaso, J Gregory},
doi = {10.1128/mSystems.00062-16.Editor},
file = {:Users/lukas/Dropbox/references/Mendeley/Bokulich2016 mockrobiota.pdf:pdf},
journal = {mSystems},
number = {5},
pages = {e00062--16},
title = {{mockrobiota: a Public Resource for Microbiome Bioinformatics Benchmarking}},
volume = {1},
year = {2016}
}
@article{Morris2018,
abstract = {Simulation studies are computer experiments which involve creating data by pseudorandom sampling. The key strength of simulation studies is the ability to understand the behaviour of statistical methods because some `truth' is known from the process of generating the data. This allows us to consider properties of methods, such as bias. While widely used, simulation studies are often poorly designed, analysed and reported. This article outlines the rationale for using simulation studies and offers guidance for design, execution, analysis, reporting and presentation. In particular, we provide: a structured approach for planning and reporting simulation studies; coherent terminology for simulation studies; guidance on coding simulation studies; a critical discussion of key performance measures and their computation; ideas on structuring tabular and graphical presentation of results; and new graphical presentations. With a view to describing current practice and identifying areas for improvement, we review 100 articles taken from Volume 34 of Statistics in Medicine which included at least one simulation study.},
archivePrefix = {arXiv},
arxivId = {1712.03198},
author = {Morris, Tim P and White, Ian R and Crowther, Michael J},
eprint = {1712.03198},
file = {:Users/lukas/Dropbox/references/Mendeley/Morris2018 Using simulation studies to evaluate statistical methods.pdf:pdf},
journal = {arXiv},
keywords = {graphics for simulation,monte carlo,simulation design,simulation reporting,simulation studies},
title = {{Using simulation studies to evaluate statistical methods}},
url = {http://arxiv.org/abs/1712.03198},
year = {2018}
}
@article{Abdelaal2018,
abstract = {Mass cytometry (CyTOF) is a valuable technology for high-dimensional analysis at the single cell level. Identification of different cell populations is an important task during the data analysis. Many clustering tools can perform this task, however, they are time consuming, often involve a manual step, and lack reproducibility when new data is included in the analysis. Learning cell types from an annotated set of cells solves these problems. However, currently available mass cytometry classifiers are either complex, dependent on prior knowledge of the cell type markers during the learning process, or can only identify canonical cell types. We propose to use a Linear Discriminant Analysis (LDA) classifier to automatically identify cell populations in CyTOF data. LDA shows comparable results with two state-of-the-art algorithms on four benchmark datasets and also outperforms a non-linear classifier such as the k-nearest neighbour clas-sifier. To illustrate its scalability to large datasets with deeply annotated cell subtypes, we apply LDA to a dataset of {\~{}}3.5 million cells representing 57 cell types. LDA has high performance on abundant cell types as well as the majority of rare cell types, and provides accurate estimates of cell type frequen-cies. Further incorporating a rejection option, based on the estimated posterior probabilities, allows LDA to identify cell types that were not encountered during training. Altogether, reproducible prediction of cell type compositions using LDA opens up possibilities to analyse large cohort studies based on mass cytometry data.},
author = {Abdelaal, Tamim and van Unen, Vincent and H{\"{o}}llt, Thomas and Koning, Frits and Reinders, Marcel J.T. and Mahfouz, Ahmed},
doi = {10.1101/316034},
file = {:Users/lukas/Dropbox/references/papers/Abdelaal2018 Predicting cell types in single cell mass cytometry data.pdf:pdf},
journal = {bioRxiv},
title = {{Predicting cell types in single cell mass cytometry data}},
url = {https://www.biorxiv.org/content/early/2018/05/07/316034?{\%}3Fcollection=},
year = {2018}
}
@article{Angers-Loustau2018,
abstract = {Next-Generation Sequencing (NGS) technologies are expected to play a crucial role in the surveillance of infectious diseases, with their unprecedented capabilities for the characterisation of genetic information underlying the virulence and antimicrobial resistance (AMR) properties of microorganisms.  In the implementation of any novel technology for regulatory purposes, important considerations such as harmonisation, validation and quality assurance need to be addressed.  NGS technologies pose unique challenges in these regards, in part due to their reliance on bioinformatics for the processing and proper interpretation of the data produced.  Well-designed benchmark resources are thus needed to evaluate, validate and ensure continued quality control over the bioinformatics component of the process.  This concept was explored as part of a workshop on "Next-generation sequencing technologies and antimicrobial resistance" held October 4-5 2017.   Challenges involved in the development of such a benchmark resource, with a specific focus on identifying the molecular determinants of AMR, were identified. For each of the challenges, sets of unsolved questions that will need to be tackled for them to be properly addressed were compiled. These take into consideration the requirement for monitoring of AMR bacteria in humans, animals, food and the environment, which is aligned with the principles of a “One Health” approach.},
author = {Angers-Loustau, Alexandre and Petrillo, Mauro and Bengtsson-Palme, Johan and Berendonk, Thomas and Blais, Burton and Chan, Kok-Gan and Coque, Teresa M. and Hammer, Paul and He{\ss}, Stefanie and Kagkli, Dafni M. and Krumbiegel, Carsten and Lanza, Val F. and Madec, Jean-Yves and Naas, Thierry and O'Grady, Justin and Paracchini, Valentina and Rossen, John W.A. and Rupp{\'{e}}, Etienne and Vamathevan, Jessica and Venturi, Vittorio and {Van den Eede}, Guy},
doi = {10.12688/f1000research.14509.1},
file = {:Users/lukas/Dropbox/references/Mendeley/Angers-Loustau2018 The challenges of designing a benchmark strategy for bioinformatics pipelines.pdf:pdf},
issn = {2046-1402},
journal = {F1000Research},
number = {v1},
pages = {459},
pmid = {30026930},
title = {{The challenges of designing a benchmark strategy for bioinformatics pipelines in the identification of antimicrobial resistance determinants using next generation sequencing technologies}},
url = {https://f1000research.com/articles/7-459/v1},
volume = {7},
year = {2018}
}
@article{MAQC2006,
author = {{MAQC Consortium}},
doi = {10.1038/nbt1239},
file = {:Users/lukas/Dropbox/references/Mendeley/MAQCConsortium2006 The MicroArray Quality Control (MAQC) project.pdf:pdf},
issn = {1309100X},
journal = {Nature Biotechnology},
number = {9},
pages = {1151--1161},
title = {{The MicroArray Quality Control (MAQC) project shows inter- and intraplatform reproducibility of gene expression measurements}},
volume = {24},
year = {2006}
}
@article{Kuffner2015,
abstract = {Amyotrophic lateral sclerosis (ALS) is a fatal neurodegenerative disease with substantial heterogeneity in its clinical presentation. This makes diagnosis and effective treatment difficult, so better tools for estimating disease progression are needed. Here, we report results from the DREAM-Phil Bowen ALS Prediction Prize4Life challenge. In this crowdsourcing competition, competitors developed algorithms for the prediction of disease progression of 1,822 ALS patients from standardized, anonymized phase 2/3 clinical trials. The two best algorithms outperformed a method designed by the challenge organizers as well as predictions by ALS clinicians. We estimate that using both winning algorithms in future trial designs could reduce the required number of patients by at least 20{\%}. The DREAM-Phil Bowen ALS Prediction Prize4Life challenge also identified several potential nonstandard predictors of disease progression including uric acid, creatinine and surprisingly, blood pressure, shedding light on ALS pathobiology. This analysis reveals the potential of a crowdsourcing competition that uses clinical trial data for accelerating ALS research and development.},
author = {K{\"{u}}ffner, Robert and Zach, Neta and Norel, Raquel and Hawe, Johann and Schoenfeld, David and Wang, Liuxia and Li, Guang and Fang, Lilly and Mackey, Lester and Hardiman, Orla and Cudkowicz, Merit and Sherman, Alexander and Ertaylan, Gokhan and Grosse-Wentrup, Moritz and Hothorn, Torsten and van Ligtenberg, Jules and MacKe, Jakob H. and Meyer, Timm and Sch{\"{o}}lkopf, Bernhard and Tran, Linh and Vaughan, Rubio and Stolovitzky, Gustavo and Leitner, Melanie L.},
doi = {10.1038/nbt.3051},
file = {:Users/lukas/Dropbox/references/Mendeley/K{\"{u}}ffner2015 Crowdsourced analysis of clinical trial data to predict amyotrophic lateral sclerosis progression.pdf:pdf},
isbn = {doi:10.1038/nbt.3051},
issn = {15461696},
journal = {Nature Biotechnology},
number = {1},
pages = {51--57},
pmid = {25362243},
title = {{Crowdsourced analysis of clinical trial data to predict amyotrophic lateral sclerosis progression}},
volume = {33},
year = {2015}
}
@article{Boulesteix2017,
abstract = {The goal of medical research is to develop interventions that are in some sense superior, with respect to patient outcome, to interventions currently in use. Similarly, the goal of research in methodological computational statistics is to develop data analysis tools that are themselves superior to the existing tools. The methodology of the evaluation of medical interventions continues to be discussed extensively in the literature and it is now well accepted that medicine should be at least partly “evidence-based”. Although we statisticians are convinced of the importance of unbiased, well-thought-out study designs and evidence-based approaches in the context of clinical research, we tend to ignore these principles when designing our own studies for evaluating statistical methods in the context of our methodological research. In this paper, we draw an analogy between clinical trials and real-data-based benchmarking experiments in methodological statistical science, with datasets playing the role of patients and methods playing the role of medical interventions. Through this analogy, we suggest directions for improvement in the design and interpretation of studies which use real data to evaluate statistical methods, in particular with respect to dataset inclusion criteria and the reduction of various forms of bias. More generally, we discuss the concept of “evidence-based” statistical research, its limitations and its impact on the design and interpretation of real-data-based benchmark experiments. We suggest that benchmark studies—a method of assessment of statistical methods using real-world datasets—might benefit from adopting (some) concepts from evidence-based medicine towards the goal of more evidence-based statistical research.},
author = {Boulesteix, Anne-Laure and Wilson, Rory and Hapfelmeier, Alexander},
doi = {10.1186/s12874-017-0417-2},
file = {:Users/lukas/Dropbox/references/Mendeley/Boulesteix2017 Towards evidence-based computational statistics.pdf:pdf},
issn = {14712288},
journal = {BMC Medical Research Methodology},
keywords = {Clinical trial,Comparison study,Good practice,Method evaluation},
number = {138},
pmid = {28888225},
title = {{Towards evidence-based computational statistics: lessons from clinical research on the role and design of real-data benchmark studies}},
volume = {17},
year = {2017}
}
@article{Soneson2013,
abstract = {Finding genes that are differentially expressed between conditions is an integral part of understanding the molecular basis of phenotypic variation. In the past decades, DNA microarrays have been used extensively to quantify the abundance of mRNA corresponding to different genes, and more recently high-throughput sequencing of cDNA (RNA-seq) has emerged as a powerful competitor. As the cost of sequencing decreases, it is conceivable that the use of RNA-seq for differential expression analysis will increase rapidly. To exploit the possibilities and address the challenges posed by this relatively new type of data, a number of software packages have been developed especially for differential expression analysis of RNA-seq data.},
author = {Soneson, Charlotte and Delorenzi, Mauro},
doi = {10.1186/1471-2105-14-91},
file = {:Users/lukas/Dropbox/references/Mendeley/Soneson2013 A comparison of methods for differential expression analysis.pdf:pdf},
isbn = {1471-2105 (Electronic)$\backslash$r1471-2105 (Linking)},
issn = {14712105},
journal = {BMC Bioinformatics},
keywords = {Differential expression,Gene expression,RNA-seq},
number = {91},
pmid = {23497356},
title = {{A comparison of methods for differential expression analysis of RNA-seq data}},
volume = {14},
year = {2013}
}
@article{Zappia2018,
abstract = {As single-cell RNA-sequencing (scRNA-seq) datasets have become more widespread the number of tools designed to analyse these data has dramatically increased. Navigating the vast sea of tools now available is becoming increasingly challenging for researchers. In order to better facilitate selection of appropriate analysis tools we have created the scRNA-tools database (www.scRNA-tools.org) to catalogue and curate analysis tools as they become available. Our database collects a range of information on each scRNA-seq analysis tool and categorises them according to the analysis tasks they perform. Exploration of this database gives insights into the areas of rapid development of analysis methods for scRNA-seq data. We see that many tools perform tasks specific to scRNA-seq analysis, particularly clustering and ordering of cells. We also find that the scRNA-seq community embraces an open-source and open-science approach, with most tools available under open-source licenses and preprints being extensively used as a means to describe methods. The scRNA-tools database provides a valuable resource for researchers embarking on scRNA-seq analysis and records the growth of the field over time.},
author = {Zappia, Luke and Phipson, Belinda and Oshlack, Alicia},
doi = {10.1371/journal.pcbi.1006245},
file = {:Users/lukas/Dropbox/references/papers/Zappia2018 Exploring the single-cell RNA-seq analysis landscape.pdf:pdf},
isbn = {1111111111},
issn = {15537358},
journal = {PLoS Computational Biology},
number = {6},
pages = {e1006245},
pmid = {29939984},
title = {{Exploring the single-cell RNA-seq analysis landscape with the scRNA-tools database}},
volume = {14},
year = {2018}
}
@article{Peng2011,
abstract = {Computational science has led to exciting new developments, but the nature of the work has exposed limitations in our ability to evaluate published findings. Reproducibility has the potential to serve as a minimum standard for judging scientific claims when full independent replication of a study is not possible.},
archivePrefix = {arXiv},
arxivId = {0901.4552},
author = {Peng, Roger D.},
doi = {10.1126/science.1213847},
eprint = {0901.4552},
file = {:Users/lukas/Dropbox/references/papers/Peng2011 Reproducible Research in Computational Science.pdf:pdf},
isbn = {1095-9203 (Electronic)$\backslash$r0036-8075 (Linking)},
issn = {10959203},
journal = {Science},
pages = {1226--1227},
pmid = {22144613},
title = {{Reproducible research in computational science}},
volume = {334},
year = {2011}
}
@article{Zheng2017b,
abstract = {Characterizing the transcriptome of individual cells is fundamental to understanding complex biological systems. We describe a droplet-based system that enables 3′ mRNA counting of up to tens of thousands of single cells per sample. Cell encapsulation in droplets takes place in {\~{}}6 minutes, with {\~{}}50{\%} cell capture efficiency, up to 8 samples at a time. The speed and efficiency allow the processing of precious samples while minimizing stress to cells. To demonstrate the system′s technical performance and its applications, we collected transcriptome data from {\~{}}¼ million single cells across 29 samples. First, we validate the sensitivity of the system and its ability to detect rare populations using cell lines and synthetic RNAs. Then, we profile 68k peripheral blood mononuclear cells (PBMCs) to demonstrate the system′s ability to characterize large immune populations. Finally, we use sequence variation in the transcriptome data to determine host and donor chimerism at single cell resolution in bone marrow mononuclear cells (BMMCs) of transplant patients. This analysis enables characterization of the complex interplay between donor and host cells and monitoring of treatment response. This high-throughput system is robust and enables characterization of diverse biological systems with single cell mRNA analysis.},
archivePrefix = {arXiv},
arxivId = {065912},
author = {Zheng, Grace X.Y. and Terry, Jessica M. and Belgrader, Phillip and Ryvkin, Paul and Bent, Zachary W. and Wilson, Ryan and Ziraldo, Solongo B. and Wheeler, Tobias D. and McDermott, Geoff P. and Zhu, Junjie and Gregory, Mark T. and Shuga, Joe and Montesclaros, Luz and Underwood, Jason G. and Masquelier, Donald A. and Nishimura, Stefanie Y. and Schnall-Levin, Michael and Wyatt, Paul W. and Hindson, Christopher M. and Bharadwaj, Rajiv and Wong, Alexander and Ness, Kevin D. and Beppu, Lan W. and Deeg, H. Joachim and McFarland, Christopher and Loeb, Keith R. and Valente, William J. and Ericson, Nolan G. and Stevens, Emily A. and Radich, Jerald P. and Mikkelsen, Tarjei S. and Hindson, Benjamin J. and Bielas, Jason H.},
doi = {10.1038/ncomms14049},
eprint = {065912},
file = {:Users/lukas/Dropbox/references/Mendeley/Zheng2016 Massively parallel digital transcriptional profiling of single cells.pdf:pdf},
isbn = {9780979806483},
issn = {20411723},
journal = {Nature Communications},
pages = {14049},
pmid = {28091601},
publisher = {Nature Publishing Group},
title = {{Massively parallel digital transcriptional profiling of single cells}},
url = {http://dx.doi.org/10.1038/ncomms14049},
volume = {8},
year = {2017}
}
@article{Sandve2013,
author = {Sandve, Geir Kjetil and Nekrutenko, Anton and Taylor, James and Hovig, Eivind},
doi = {10.1371/journal.pcbi.1003285},
file = {:Users/lukas/Dropbox/references/papers/Sandve2013 Ten Simple Rules for Reproducible Computational Research.pdf:pdf},
isbn = {1553-7358 (Electronic)$\backslash$r1553-734X (Linking)},
issn = {1553734X},
journal = {PLoS Computational Biology},
number = {10},
pages = {e1003285},
pmid = {24204232},
title = {{Ten Simple Rules for Reproducible Computational Research}},
volume = {9},
year = {2013}
}
@article{Ewing2015,
abstract = {The detection of somatic mutations from cancer genome sequences is key to understanding the genetic basis of disease progression, patient survival and response to therapy. Benchmarking is needed for tool assessment and improvement but is complicated by a lack of gold standards, by extensive resource requirements and by difficulties in sharing personal genomic information. To resolve these issues, we launched the ICGC-TCGA DREAM Somatic Mutation Calling Challenge, a crowdsourced benchmark of somatic mutation detection algorithms. Here we report the BAMSurgeon tool for simulating cancer genomes and the results of 248 analyses of three in silico tumors created with it. Different algorithms exhibit characteristic error profiles, and, intriguingly, false positives show a trinucleotide profile very similar to one found in human tumors. Although the three simulated tumors differ in sequence contamination (deviation from normal cell sequence) and in subclonality, an ensemble of pipelines outperforms the best individual pipeline in all cases. BAMSurgeon is available at https://github.com/adamewing/bamsurgeon/.},
author = {Ewing, Adam D. and Houlahan, Kathleen E. and Hu, Yin and Ellrott, Kyle and Caloian, Cristian and Yamaguchi, Takafumi N. and Bare, J. Christopher and P'ng, Christine and Waggott, Daryl and Sabelnykova, Veronica Y. and {ICGC-TCGA DREAM Somatic Mutation Calling Challenge participants} and Kellen, Michael R. and Norman, Thea C. and Haussler, David and Friend, Stephen H. and Stolovitzky, Gustavo and Margolin, Adam A. and Stuart, Joshua M. and Boutros, Paul C.},
doi = {10.1038/nmeth.3407},
file = {:Users/lukas/Dropbox/references/Mendeley/Ewing2015 Combining tumor genome simulation with crowdsourcing.pdf:pdf},
isbn = {1548-7091},
issn = {15487105},
journal = {Nature Methods},
number = {7},
pages = {623--630},
pmid = {25984700},
title = {{Combining tumor genome simulation with crowdsourcing to benchmark somatic single-nucleotide-variant detection}},
volume = {12},
year = {2015}
}
@article{Saito2015,
abstract = {Binary classifiers are routinely evaluated with performance measures such as sensitivity and specificity, and performance is frequently illustrated with Receiver Operating Characteristics (ROC) plots. Alternative measures such as positive predictive value (PPV) and the associated Precision/Recall (PRC) plots are used less frequently. Many bioinformatics studies develop and evaluate classifiers that are to be applied to strongly imbalanced datasets in which the number of negatives outweighs the number of positives significantly. While ROC plots are visually appealing and provide an overview of a classifier's performance across a wide range of specificities, one can ask whether ROC plots could be misleading when applied in imbalanced classification scenarios. We show here that the visual interpretability of ROC plots in the context of imbalanced datasets can be deceptive with respect to conclusions about the reliability of classification performance, owing to an intuitive but wrong interpretation of specificity. PRC plots, on the other hand, can provide the viewer with an accurate prediction of future classification performance due to the fact that they evaluate the fraction of true positives among positive predictions. Our findings have potential implications for the interpretation of a large number of studies that use ROC plots on imbalanced datasets.},
author = {Saito, Takaya and Rehmsmeier, Marc},
doi = {10.1371/journal.pone.0118432},
file = {:Users/lukas/Dropbox/references/Mendeley/Saito2015 The Precision-Recall Plot Is More Informative than the ROC Plot.pdf:pdf},
isbn = {0022-1333 (Print)},
issn = {19326203},
journal = {PLoS ONE},
number = {3},
pages = {e0118432},
pmid = {25738806},
title = {{The precision-recall plot is more informative than the ROC plot when evaluating binary classifiers on imbalanced datasets}},
volume = {10},
year = {2015}
}
@article{Levine2015,
abstract = {Summary Acute myeloid leukemia (AML) manifests as phenotypically and functionally diverse cells, often within the same patient. Intratumor phenotypic and functional heterogeneity have been linked primarily by physical sorting experiments, which assume that functionally distinct subpopulations can be prospectively isolated by surface phenotypes. This assumption has proven problematic, and we therefore developed a data-driven approach. Using mass cytometry, we profiled surface and intracellular signaling proteins simultaneously in millions of healthy and leukemic cells. We developed PhenoGraph, which algorithmically defines phenotypes in high-dimensional single-cell data. PhenoGraph revealed that the surface phenotypes of leukemic blasts do not necessarily reflect their intracellular state. Using hematopoietic progenitors, we defined a signaling-based measure of cellular phenotype, which led to isolation of a gene expression signature that was predictive of survival in independent cohorts. This study presents new methods for large-scale analysis of single-cell heterogeneity and demonstrates their utility, yielding insights into AML pathophysiology.},
author = {Levine, Jacob H and Simonds, Erin F and Bendall, Sean C and Davis, Kara L and Amir, El-ad D and Tadmor, Michelle D and Litvin, Oren and Fienberg, Harris G and Jager, Astraea and Zunder, Eli R and Finck, Rachel and Gedman, Amanda L and Radtke, Ina and Downing, James R and Pe'er, Dana and Nolan, Garry P},
doi = {10.1016/j.cell.2015.05.047},
file = {:Users/lukas/Dropbox/references/papers/Levine2015{\_}PhenoGraph.pdf:pdf},
isbn = {0092-8674},
issn = {10974172},
journal = {Cell},
pages = {184--197},
pmid = {26095251},
publisher = {Elsevier Inc.},
title = {{Data-Driven Phenotypic Dissection of AML Reveals Progenitor-like Cells that Correlate with Prognosis}},
url = {http://dx.doi.org/10.1016/j.cell.2015.05.047},
volume = {162},
year = {2015}
}
@article{Boulesteix2018,
author = {Boulesteix, Anne-Laure and Binder, Harald and Abrahamowicz, Michal and Sauerbrei, Willi},
doi = {10.1002/bimj.201700129},
file = {:Users/lukas/Dropbox/references/Mendeley/Boulesteix2018 On the necessity and design of studies comparing statistical method.pdf:pdf},
issn = {03233847},
journal = {Biometrical Journal},
pages = {216--218},
title = {{On the necessity and design of studies comparing statistical methods}},
url = {http://doi.wiley.com/10.1002/bimj.201700129},
volume = {60},
year = {2018}
}
@article{Spidlen2012,
abstract = {Data associated with peer-reviewed manuscripts should be easily available and accessible; however, the rapid expansion of flow cytometry (FCM) applications has outpaced the development of tools for storage, analysis, and data representation (1–3). In addition, data associated with peer-reviewed manuscripts are rarely available publicly and even then are usually not stored with explicit links to experimental metadata, such as data analyses procedures, experimental conditions used, or information about the samples processed. This link between data and metadata is crucial as it facilitates the understanding of analysis approaches and reproducible research. Having datasets linked to figures and summaries through a detailed explanation of the sample processing and analysis pipeline allows other scientists to ask additional questions and build upon the published findings.},
author = {Spidlen, Josef and Breuer, Karin and Rosenberg, Chad and Kotecha, Nikesh and Brinkman, Ryan R},
doi = {10.1002/cyto.a.22106},
file = {:Users/lukas/Dropbox/references/papers/Spidlen2012{\_}FlowRepository.pdf:pdf},
issn = {15524922},
journal = {Cytometry Part A},
pages = {727--731},
pmid = {22887982},
title = {{FlowRepository: A resource of annotated flow cytometry datasets associated with peer-reviewed publications}},
volume = {81A},
year = {2012}
}
@article{Baruzzo2017,
abstract = {AnAlysis nAture methods | ADVANCE ONLINE PUBLICATION | Alignment is the first step in most rnA-seq analysis pipelines, and the accuracy of downstream analyses depends heavily on it. unlike most steps in the pipeline, alignment is particularly amenable to benchmarking with simulated data. We performed a comprehensive benchmarking of 4 common splice-aware aligners for base, read, and exon junction-level accuracy and compared default with optimized parameters. We found that performance varied by genome complexity, and accuracy and popularity were poorly correlated. the most widely cited tool underperforms for most metrics, particularly when using default settings.},
archivePrefix = {arXiv},
arxivId = {15334406},
author = {Baruzzo, Giacomo and Hayer, Katharina E. and Kim, Eun Ji and {Di Camillo}, Barbara and Fitzgerald, Garret A. and Grant, Gregory R.},
doi = {10.1038/nmeth.4106},
eprint = {15334406},
file = {:Users/lukas/Dropbox/references/Mendeley/Baruzzo2017 Simulation-based comprehensive benchmarking of RNA-seq aligners.pdf:pdf},
isbn = {1548-7105 (Electronic) 1548-7091 (Linking)},
issn = {15487105},
journal = {Nature Methods},
number = {2},
pages = {135--139},
pmid = {27941783},
title = {{Simulation-based comprehensive benchmarking of RNA-seq aligners}},
volume = {14},
year = {2017}
}
@article{Weber2018,
abstract = {High-dimensional flow and mass cytometry allow cell types and states to be characterized in great detail by measuring expression levels of more than 40 protein markers per cell. Here, we present diffcyt, a new computational framework for differential discovery analyses in these datasets, based on (i) high-resolution clustering and (ii) empirical Bayes moderated tests adapted from transcriptomics. Our approach provides improved statistical performance, including for rare cell populations, along with flexible experimental designs and fast runtimes in an open-source framework.},
author = {Weber, Lukas M and Nowicka, Malgorzata and Soneson, Charlotte and Robinson, Mark D},
doi = {10.1101/349738},
file = {:Users/lukas/Dropbox/references/papers/Weber2018{\_}diffcyt.pdf:pdf},
journal = {bioRxiv},
title = {{diffcyt: Differential discovery in high-dimensional cytometry via high-resolution clustering}},
url = {https://www.biorxiv.org/content/early/2018/06/18/349738},
year = {2018}
}
@article{Weirauch2013,
abstract = {Genomic analyses often involve scanning for potential transcription factor (TF) binding sites using models of the sequence specificity of DNA binding proteins. Many approaches have been developed to model and learn a protein's DNA-binding specificity, but these methods have not been systematically compared. Here we applied 26 such approaches to in vitro protein binding microarray data for 66 mouse TFs belonging to various families. For nine TFs, we also scored the resulting motif models on in vivo data, and found that the best in vitro-derived motifs performed similarly to motifs derived from the in vivo data. Our results indicate that simple models based on mononucleotide position weight matrices trained by the best methods perform similarly to more complex models for most TFs examined, but fall short in specific cases ({\textless}10{\%} of the TFs examined here). In addition, the best-performing motifs typically have relatively low information content, consistent with widespread degeneracy in eukaryotic TF sequence preferences.},
author = {Weirauch, Matthew T. and Cote, Atina and Norel, Raquel and Annala, Matti and Zhao, Yue and Riley, Todd R. and Saez-Rodriguez, Julio and Cokelaer, Thomas and Vedenko, Anastasia and Talukder, Shaheynoor and {DREAM5 Consortium} and Bussemaker, Harmen J. and Morris, Quaid D. and Bulyk, Martha L. and Stolovitzky, Gustavo and Hughes, Timothy R.},
doi = {10.1038/nbt.2486},
file = {:Users/lukas/Dropbox/references/Mendeley/Weirauch2013 Evaluation of methods for modeling transcription factor sequence specificity.pdf:pdf},
isbn = {1546-1696 (Electronic)$\backslash$n1087-0156 (Linking)},
issn = {10870156},
journal = {Nature Biotechnology},
number = {2},
pages = {126--134},
pmid = {23354101},
publisher = {Nature Publishing Group},
title = {{Evaluation of methods for modeling transcription factor sequence specificity}},
url = {http://dx.doi.org/10.1038/nbt.2486},
volume = {31},
year = {2013}
}
@article{Kolesnikov2015,
abstract = {The ArrayExpress Archive of Functional Genomics Data (http://www.ebi.ac.uk/arrayexpress) is an international functional genomics database at the European Bioinformatics Institute (EMBL-EBI) recommended by most journals as a repository for data supporting peer-reviewed publications. It contains data from over 7000 public sequencing and 42,000 array-based studies comprising over 1.5 million assays in total. The proportion of sequencing-based submissions has grown significantly over the last few years and has doubled in the last 18 months, whilst the rate of microarray submissions is growing slightly. All data in ArrayExpress are available in the MAGE-TAB format, which allows robust linking to data analysis and visualization tools and standardized analysis. The main development over the last two years has been the release of a new data submission tool Annotare, which has reduced the average submission time almost 3-fold. In the near future, Annotare will become the only submission route into ArrayExpress, alongside MAGE-TAB format-based pipelines. ArrayExpress is a stable and highly accessed resource. Our future tasks include automation of data flows and further integration with other EMBL-EBI resources for the representation of multi-omics data.},
author = {Kolesnikov, Nikolay and Hastings, Emma and Keays, Maria and Melnichuk, Olga and Tang, Y. Amy and Williams, Eleanor and Dylag, Miroslaw and Kurbatova, Natalja and Brandizi, Marco and Burdett, Tony and Megy, Karyn and Pilicheva, Ekaterina and Rustici, Gabriella and Tikhonov, Andrew and Parkinson, Helen and Petryszak, Robert and Sarkans, Ugis and Brazma, Alvis},
doi = {10.1093/nar/gku1057},
file = {:Users/lukas/Dropbox/references/papers/Kolesnikov2015 ArrayExpress update - simplifying data submissions.pdf:pdf},
isbn = {0305-1048},
issn = {13624962},
journal = {Nucleic Acids Research},
pages = {D1113--D1116},
pmid = {25361974},
title = {{ArrayExpress update - simplifying data submissions}},
volume = {43},
year = {2015}
}
