%%%%%%%%%%%%%%%%%%%
% Document settings
%%%%%%%%%%%%%%%%%%%

\documentclass[12pt, a4paper]{article}
\usepackage[top = 2.25cm, left = 2.25cm, right = 2.25cm, bottom = 2.25cm]{geometry}
\usepackage[utf8]{inputenc}

\usepackage{setspace}  % line spacing
\usepackage{lineno}  % line numbers

%\usepackage{cite}  % citations
\usepackage[square, numbers, sort&compress]{natbib}

\usepackage{float}  % figure/table placement
\usepackage{graphicx}  % figures

% disable ligatures
\usepackage{microtype}
\DisableLigatures[f]{encoding = *, family = *}

% include dots in table of contents
%\usepackage{tocloft}
%\renewcommand{\cftsecleader}{\cftdotfill{\cftdotsep}}

% avoid single lines of text at top or bottom of page
\widowpenalty 10000
\clubpenalty 10000

% text layout
\sloppy
\setlength{\parindent}{1cm}

% captions: bold 'Figure #' heading and end with full stop; left-justified
\usepackage[aboveskip = 10pt, labelfont = bf, labelsep = period, singlelinecheck = off, font = small]{caption}

% links
\usepackage{hyperref}
\hypersetup{colorlinks, citecolor = black, linkcolor = black, urlcolor = blue}

\usepackage[shortcuts]{extdash}  % allow line breaks at hyphens

% subfigures
\usepackage{subcaption}

% leave date blank
%\date{}

% adjust font style for section headings
\usepackage{titlesec}
\titleformat*{\section}{\large\bfseries\flushleft}

% to adjust indent width in enumerated list
\usepackage{enumitem}




%%%%%%%%%%
% Document
%%%%%%%%%%

\begin{document}




%%%%%%%%%%%%
% Title page
%%%%%%%%%%%%

\title{\Large{\textbf{
Essential guidelines for computational \\method benchmarking
}}{\vskip 1cm}}

\author{
Lukas M. Weber$^{1,2}$,
Wouter Saelens$^{3,4}$,\\
Robrecht Cannoodt$^{3,4}$,
Charlotte Soneson$^{1,2,\dagger}$,\\
Yvan Saeys$^{3,4,*}$,
Mark D. Robinson$^{1,2,*}$\\ \\ \\
\footnotesize{$^1$Institute of Molecular Life Sciences, University of Zurich, Zurich, Switzerland}\\
\footnotesize{$^2$SIB Swiss Institute of Bioinformatics, University of Zurich, Zurich, Switzerland}\\
\footnotesize{$^3$Data Mining and Modelling for Biomedicine, VIB Center for Inflammation Research, Ghent, Belgium}\\
\footnotesize{$^4$Department of Applied Mathematics, Computer Science and Statistics, Ghent University, Ghent, Belgium}\\
\footnotesize{$\dagger$Current address: Friedrich Miescher Institute for Biomedical Research, Basel, Switzerland}\\ \\
\small{$^*$Corresponding authors}\\ \\ \\ \\
}

\date{\today}



\begin{titlepage}

\maketitle

% no page number (note: this line must be after \maketitle)
\thispagestyle{empty}

\end{titlepage}




%%%%%%%%%%%%%%%%%%%
\section*{Abstract}
%%%%%%%%%%%%%%%%%%%

In computational biology and other sciences, researchers are frequently faced with a choice between several similar computational methods for performing data analyses. Benchmarking studies aim to rigorously compare and evaluate the performance of different methods using well-characterized benchmark datasets, to determine the strengths of each method or to provide recommendations regarding the best choice of method for an analysis. However, benchmarking studies must be carefully designed and implemented to provide accurate and unbiased results. Here, we summarize key practical guidelines and recommendations for performing high-quality benchmarking analyses, based on our own experiences in computational biology.




%%%%%%%%%%%%%%%%%%%%%%%
\section*{Introduction}
%%%%%%%%%%%%%%%%%%%%%%%

Many fields of computational research are characterized by a fast growing number of available methods for data analysis. For example, as of mid 2018, more than 230 methods are available for analyzing data from single-cell RNA-sequencing experiments \citep{Zappia2018}. For experimental researchers and method users, this represents both an opportunity and a challenge, since method choice can significantly affect conclusions.

Benchmarking studies are carried out by computational researchers to compare the performance of different methods, using standardized reference datasets and a range of evaluation criteria. Benchmarks may be performed by authors of new methods to demonstrate performance improvements or other advantages; by independent groups interested in systematically comparing existing methods; or organized as community challenges. `Neutral' benchmarking studies performed independently of method development are especially valuable for the research community, since there is no perception of bias \citep{Boulesteix2018, Boulesteix2013}.

From our experiences conducting various benchmarking studies in computational biology, we have learned several key lessons that we aim to synthesize in this review. A number of previous studies have addressed this topic from a range of perspectives, including: the design of real-data benchmarking studies by analogy to evidence-based medicine \citep{Boulesteix2017} and in terms of statistical hypothesis testing \citep{Boulesteix2015b}; guidance on the design of simulation studies \citep{Morris2018}; recommendations for reducing `overoptimistic' reporting when benchmarking newly developed methods \citep{Boulesteix2015a}; arguments stressing the need for neutral benchmarking studies \citep{Boulesteix2013}; a detailed discussion of benchmarking design for clustering methods \citep{VanMechelen2018}; an overview of challenges involved in benchmarking in the context of methods for identifying antimicrobial resistance determinants by sequencing \citep{Angers-Loustau2018}; and, commentary on benchmarking design in general \citep{Zheng2017a}.

Our aim is to complement these previous studies by providing a summary of essential guidelines and recommendations for designing benchmarking analyses. Our target audience consists of computational researchers who are interested in performing a benchmarking study, or who have already begun one. Our review spans the full `pipeline' of benchmarking, from defining the purpose and scope to reproducible research best practices during publication. The review is structured as a series of guidelines, which are summarized in Box 1 and each explained in detail in the corresponding text section. We use examples from computational biology; however, we expect that most arguments apply equally to other fields. We hope that these practical guidelines will continue the discussion on benchmarking design, as well as assisting computational researchers to design and implement rigorous, informative, and unbiased benchmarking analyses.




% Box

\vskip 1cm

\setlength{\fboxsep}{4mm}
\noindent\fbox{\begin{minipage}{0.95\textwidth}
\begin{flushleft}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Box 1: Summary of guidelines}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The guidelines in this review can be summarized in the form of the following set of recommendations. Each recommendation is discussed in more detail in the corresponding section in the text.

\begin{enumerate}[leftmargin = 8mm]

\item Define the purpose and scope of the benchmark.
\item Include all relevant methods.
\item Select (or design) representative datasets.
\item Evaluate and rank methods according to key quantitative performance metrics.
\item Evaluate runtimes and computational requirements, as well as user-friendliness, documentation, and other qualitative measures.
\item Record parameter settings, and environment or session information.
\item Interpret results and provide guidelines or recommendations from both user and method developer perspectives.
\item Distribute results in an accessible format.
\item Design the benchmark to enable future extensions.
\item Follow reproducible research best practices by making all code and data publicly available.

\end{enumerate}

\end{flushleft}
\end{minipage}}

\vskip 1.5cm




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Defining the purpose and scope}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The purpose of a benchmark will fundamentally guide the design and implementation. The purpose and scope should be clearly defined at the beginning of the study, to ensure that results are meaningful and that time and effort are not lost on unnecessary work.

In general, we can define three broad types of benchmarking studies: (i) those by method developers, to demonstrate the merits of their approach (e.g., \citep{Weber2018, Nowicka2016, Levine2015, Zhou2014, Law2014}); (ii) those performed independently, to systematically compare methods for a certain analysis, either conducted directly (e.g., \citep{Duo2018, Saelens2018a, Saelens2018b, Soneson2018, Weber2016, Baruzzo2017, Kanitz2015, Soneson2013, Rapaport2013, Dillies2012}) or in collaboration with method authors (e.g., \citep{Sage2015}); or, (iii) those organized in the form of a community challenge (e.g., by the DREAM \citep{Weirauch2013, Costello2014, Kuffner2015, Ewing2015, Hill2016}, FlowCAP \citep{Aghaeepour2013, Aghaeepour2016}, and MAQC/SEQC \citep{MAQC2006, MAQC2010, SEQCMAQCIII2014} consortia).

A crucial question to consider is how detailed the benchmarking should be. For an independent benchmarking, a high level of detail across all methods is critical, including evaluations according to performance metrics, as well as peripheral measures such as user-friendliness and documentation quality. Ideally, the evaluations should test a range of values for major parameters for each method. By contrast, when introducing a new method, the focus of the benchmark will be on evaluating the relative merits of the new method, which may be sufficiently achieved by comparing against a single set of parameters (e.g., default parameters) for competing methods.

Finally, results should be summarized in the context of the original purpose and scope of the benchmark. For example, for an independent benchmarking, from the perspective of method users, it will be useful to provide guidelines regarding the best-performing methods. For method developers, it is useful to highlight limitations of current methods that can be improved during future method development work.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Selection of methods}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The selection of methods to include in the benchmark will be guided by the purpose and scope of the study. For example, for an independent benchmarking, the aim should be to include all available methods for a certain type of analysis, to provide readers with comprehensive information. In this case, the publication describing the benchmark will also function as a review of the literature, so it will be useful to include information describing each method; e.g., in the form of a summary table (see Table 1 in \citep{Weber2016} or Table 1 in \citep{Saelens2018b}). Alternatively, it may make sense to include only a subset of methods, by defining inclusion criteria: e.g., all methods that (i) provide freely available software implementations; (ii) are available for commonly used operating systems; and, (iii) can successfully be installed without errors. If the benchmark is organized as a community challenge, the selection of methods will be determined by the participants. In this case, it is important to communicate the initiative widely, to ensure all method authors are aware of it; this can be facilitated by organizing the initiative through an established network, such as DREAM challenges.

When developing a new method, it is generally sufficient to select a representative subset of existing methods to compare against. For example, this could consist of the current best-performing methods (if known), a simple `baseline' method, and any methods that are widely used. The selection of competing methods should ensure an accurate and unbiased assessment of the relative merits of the new approach, compared to the current `state-of-the-art'. In fast-moving fields, method developers should be prepared to update their benchmarks as new methods emerge.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Selection (or design) of datasets}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The selection of reference datasets is a critical design choice. If suitable publicly accessible datasets cannot be found in the literature, they will need to be generated or constructed, either experimentally or by simulation. Including a variety of datasets ensures that methods can be evaluated under a wide range of conditions. In general, reference datasets can be grouped into two main categories: simulated (or synthetic) and real (or experimental).

Simulated data have the advantage that a known `truth' (or `ground truth') can easily be introduced; e.g., whether a gene is truly differentially expressed. Quantitative performance metrics, such as sensitivity to recover the known truth, can then be used to rank methods. However, it is important to carefully design a simulation that accurately reflects various properties of real data; e.g., this can be achieved by inspecting the empirical summaries of both simulated and real datasets (e.g., using automated tools \citep{Soneson2017}). The set of empirical summaries to use is context-specific, e.g., for single-cell RNA-sequencing, dropout profiles and dispersion-mean relationships should be compared \citep{Soneson2018}; for DNA methylation, correlation patterns among neighboring CpG sites should be investigated \citep{Korthauer2018}; for comparing mapping algorithms, error profiles of the sequencing platforms should be considered [COMMENT: need reference]. Simplified simulations may also be useful to evaluate a new method under specific assumptions.

Experimental data, by definition, reflect the properties of real data, but often do not contain a ground truth, making it difficult to calculate performance metrics. Instead, methods may be evaluated by comparing them against each other, or against a current widely accepted method or `gold standard' (e.g., `manual gating' to define cell populations in high-dimensional cytometry data \citep{Weber2016, Aghaeepour2013}, or fluorescence in situ hybridization (FISH) to validate absolute copy number predictions \citep{Zheng2017a}). In some cases, it is also possible to design experimental datasets containing an approximate ground truth. Examples include: (i) `spiking in' synthetic RNA molecules at known relative concentrations \citep{Jiang2011} in RNA-sequencing experiments (e.g., \citep{Garalde2018, SEQCMAQCIII2014}); (ii) large-scale validation of gene expression measurements by quantitative polymerase chain reaction (qPCR) (e.g., \citep{SEQCMAQCIII2014}); (iii) using genes located on sex chromosomes as a proxy for silencing of DNA methylation status (e.g., \citep{Law2014, Fang2012}); or, (iv) using fluorescence-activated cell sorting (FACS) to sort cells into known subpopulations prior to single-cell RNA-sequencing (e.g., \citep{TabulaMuris2018, Zheng2017b}) [COMMENT: also some of the datasets in `conquer'? these have truly independent labels]. Alternatively, experimental datasets may be evaluated qualitatively, for example by judging whether each method can recover previous discoveries; however, this strategy may not enable a simple ranking of methods.

A further technique is to design `semi-simulated' datasets, which combine real experimental data with an `in silico' (i.e., computational) spike-in signal; e.g., by combining cells or genes from healthy or `null' samples with a subset of cells or genes from samples expected to contain a true differential signal \citep{Arvaniti2017, Weber2018, Rigaill2018}. This strategy can create datasets containing both realistic levels of random variability [COMMENT: are we sure? I always worry that spiking in cells at fixed percentages also doesn't capture the variability] and a known ground truth, thus allowing quantitative performance metrics to be calculated.

Overall, there is no perfect reference dataset. Simulated data may not capture all properties of real data, and even when experimental data include an approximate ground truth, the correct level of variability may not be easily represented. For example, it can be difficult to ensure that the variability of spiked-in material is appropriate, or that independent measurements on cell line data represent a relevant scenario for experiments on outbred populations. In our view, the key to benchmarking is diversity of evaluations, i.e., using a range of metrics and datasets that span the range of those that might be encountered in practice.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Evaluation criteria: quantitative performance metrics}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Ranking of methods will generally rely on one or more quantitative performance metrics. The choice of metrics depends on the types of analyses under consideration. For example, for classification methods (e.g., using reference datasets with a ground truth or gold standard), metrics include the true positive rate (TPR; sensitivity or recall), false positive rate (FPR; $1 -$ specificity), and false discovery rate (FDR). For clustering methods, common metrics include the F1 score, adjusted Rand index, precision, and recall (sensitivity) (e.g., \citep{Duo2018, Weber2016, Aghaeepour2013}), which can be calculated at the cluster level or averaged (and optionally weighted) across clusters. These types of metrics return a single value between 0 and 1 (or -1 and 1), which can be easily used to rank methods. For detection methods, metrics can be compared visually to capture the tradeoff between sensitivity and specificity, e.g., using receiver operating characteristic (ROC) curves (TPR vs.\ FPR), TPR vs.\ FDR curves, or precision-recall (PR) curves; for some imbalanced datasets, PR curves have been shown to be more informative than ROC curves \citep{Saito2015}. These can also be summarized as a single number, such as area under the ROC curve (AUROC or AUC ROC) or area under the PR curve (AUC PR). Figure \ref{fig:metrics}a provides an overview of commonly used metrics, and Figure \ref{fig:metrics}b displays examples of several visualizations.

For methods that return continuous values (e.g., estimates of effect size), metrics include the root mean square error (RMSE), distance measures, and correlation. For complex data types, custom metrics may need to be developed (e.g., \citep{Saelens2018b}). When designing custom metrics, it is important to assess their robustness and usefulness across a range of prediction values. If multiple metrics are considered, the final rankings will need to be combined or weighted to produce an overall ranking.

Additional metrics that do not rely on a ground truth or gold standard include measures of stability, stochasticity, and robustness. These measures may be quantified by running methods multiple times using different inputs or subsampled data. For non-deterministic methods (e.g., with random starts or stochastic optimization), variability in results when running methods with different `random seeds' should be characterized. In addition, in the absence of ground truth, `null comparisons' can often be constructed by randomizing group labels such that datasets do not contain any true signal; analyses can provide information on expected error rates (e.g., \citep{Weber2018, Law2014}). However, null comparisons must be designed carefully to avoid confounding by batch or population structure.




% Figure

\vskip 5mm

\begin{figure}[H]
\begin{center}
\includegraphics[width=0.5\textwidth]{../../figures/benchmarking_metrics_schematic}
\includegraphics[width=\textwidth]{../../figures/metrics_examples}
\end{center}
\caption{Summary and examples of performance metrics. (a) Overview of main categories of commonly used performance metrics, including examples (gray boxes). (b) Examples of popular visualizations of quantitative performance metrics for classification methods, using reference datasets with a ground truth or gold standard: (i) ROC curves, (ii) TPR-FDR curves (circles represent observed TPR and FDR at typical FDR cutoffs of 1\%, 5\%, and 10\%; filled circles indicate that the observed FDR is lower than or equal to the imposed cutoff), and (iii) PR curves. [COMMENT: do we have a dataset that doesn't make edgeR and DESeq2 look bad, and showing filled circles?]. Visualizations in (b) generated using iCOBRA R/Bioconductor package and example data \citep{Soneson2016}.}
\label{fig:metrics}
\end{figure}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Evaluation criteria: other measures}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In addition to the main quantitative performance metrics, methods should also be evaluated according to secondary measures including runtime, scalability, and other computational requirements, as well as qualitative aspects such as user-friendliness and documentation quality.

In our experience, runtimes and scalability can vary enormously between methods; e.g., runtimes for different clustering algorithms have ranged from minutes to days for the same analysis task on the same datasets \citep{Weber2016}. Similarly, memory and other computational requirements can vary widely. Depending on the scope of the benchmark, runtimes and scalability may be investigated systematically, e.g., by varying the number of cells in a single-cell RNA-sequencing dataset. In many cases, there is a tradeoff between performance and computational requirements. In practice, if computational requirements for a top-performing method are prohibitive (e.g., requiring a high-performance server instead of a standard laptop), then a lower-ranked method may be preferred by some users.

User-friendliness, installation procedures, and documentation quality can also be highly variable. These aspects are crucial for users without extensive computational training, which includes many experimental researchers. Methods that can be installed as packages from standard repositories or using package managers (such as CRAN or Bioconductor for R, or PyPI and pip for Python) are much more accessible than methods that are only available as custom scripts, or source code that must be compiled. [COMMENT: I feel this is too negative, if people use standard compilers and reasonable dependencies, I don't see a problem with compiling code - Heng Li's software is an example]. Methods available as packages from GitHub or as pre-built binaries provide an intermediate level of accessibility. Availability across platforms (Windows, Mac, and Linux) and within popular programming languages for data analysis (R and Python) are also important. Availability of graphical user interfaces (GUIs) extends accessibility to users without command-line experience; however, methods that are only available via GUIs hinder reproducibility and are difficult to include in a systematic benchmark, since scripts cannot be used.

For many users, freely available and open source software will be preferred. Freely available software ensures broad accessibility and makes it easier to interact with collaborators, while open source software can be checked for bugs or adapted by experienced users. From the developer perspective, code quality and use of software development best practices such as unit testing and continuous integration are also important factors. Similarly, adherence to commonly used data formats (e.g., GFF/GTF files for describing genomic features, BAM/SAM files for sequence alignment data, or FCS files for flow or mass cytometry data) greatly improves accessibility and extendability. Evidence of active package maintenance and recent bug fixes provide users with confidence that unexpected problems are likely to be addressed.

High-quality documentation is critical, including help pages for individual functions as well as extended tutorials or examples demonstrating how to run complete analysis pipelines. Ideally, documentation should be continually evaluated; e.g., as done automatically by repositories such as Bioconductor.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Parameters and software environment}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Parameter settings can have a crucial impact on performance. For many methods, tuning parameters to optimal values requires significant effort. Depending on the scope of the benchmark, a range of parameter values may be tested for each method; e.g., in a `grid search' strategy to optimize across multiple parameters. [COMMENT: what is being optimized exactly? and how? I worry here about overfitting.] For other methods or for smaller benchmarks, default parameters may be adequate. However, caution is required to avoid introducing bias by expending more `researcher degrees of freedom' on tuning a preferred method \citep{Hu2018b}.

Final parameter values should be recorded, to ensure that readers have complete information. For methods that are run using scripts, parameter values will also be recorded within the scripts, which helps avoid errors and facilitates reproducibility. For methods only available via graphical interfaces, parameters must be recorded manually, for example in a spreadsheet. Reproducible workflow frameworks, such as the Galaxy platform \citep{Afgan2018}, are also helpful when using graphical tools. A summary table of parameter values can be published as supplementary information along with the publication describing the benchmark (e.g., Supporting Information Table S1 in \citep{Weber2016}).

Software versions can also influence results, especially if updates include major changes to methodology. In general, final results should be based on the latest available versions, and software versions recorded. For example, the command \texttt{sessionInfo()} in R gives a complete summary of package versions, as well as the version of R and the operating system.

Automated workflow management tools and specialized tools for organizing benchmarks provide sophisticated options for setting up benchmarks and recording software environments, package versions, and parameter values. Examples include SummarizedBenchmark \citep{Kimes2018}, DataPackageR \citep{Finak2018}, workflowr \citep{Blischak2018}, and Dynamic Statistical Comparisons \citep{Wang2018}. More general tools for managing computational workflows, including Snakemake \citep{Koster2012} and Bioconda \citep{Gruning2018}, can be customized to capture runtime information (e.g., versions). Finally, containerization tools such as Docker and Singularity may be used to completely encapsulate a software environment for each method, preserving the package version as well as dependency packages and the operating system, and facilitating further distribution of methods to end users.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Interpretation, guidelines, and recommendations}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

To make the benchmark as useful as possible for readers, the results must be clearly interpreted and summarized from the perspective of the intended audience. For method users, results can be summarized in the form of guidelines or recommendations regarding the best choice of method. This will often include an overall ranking of methods, which can be presented using visualizations (e.g., Figure \ref{fig:metrics}b) or summary tables (e.g., \citep{Aghaeepour2013, Weber2016}). If several key performance metrics were used, more complex visualizations are also informative; e.g., flow charts to guide the optimal choice of method for different analyses \citep{Saelens2018b}. The interpretation may involve biological or other domain knowledge to establish the scientific significance of the results; e.g., the practical significance of differences between rankings for the top methods. For method developers, the conclusions may include guidelines for the future development of methods; e.g., identifying limitations of current methods and highlighting possible areas for future work.

[COMMENT: I found this section a bit `fluffy'; it basically says `show your results in a table or plot'. Is there some more meat to it?]




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Publication and distribution of results}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Presenting results in an accessible format ensures that readers will be able to find them and apply them to their work. In academic research, results will usually be published as a peer-reviewed article. For an independent benchmark, the benchmark will be the main focus of the paper. For a benchmark performed to introduce a new method, the benchmark results will form one part of the exposition. We highly recommend publishing a preprint on a recognized preprint server prior to peer review (e.g., bioRxiv or arXiv) to speed up distribution of results and solicit additional feedback; at publication time, open access options will broaden accessibility.

For extensive benchmarks, it is also useful to create online resources to enable readers to interactively explore the benchmark results; and in particular, to distribute raw, pre-processed, and/or results data (e.g., \citep{Soneson2018, Soneson2016, Wiwie2015, Bokulich2016, Conchuir2015}) in accessible formats. In R, the Shiny framework provides a convenient system for building interactive websites. Automated workflow management tools (e.g., workflowr \citep{Blischak2018}) also provide streamlined options for publishing results. Figure \ref{fig:shiny} displays an example of an interactive website to explore the results of a benchmark \citep{Saelens2018b}, created using Shiny.




% Figure

\vskip 5mm

\begin{figure}[H]
\begin{center}
\includegraphics[width=\textwidth]{../../figures/shiny_example}
\end{center}
\caption{Example of an interactive website allowing users to explore the results of a benchmarking study \citep{Saelens2018b}. This website was created using the Shiny framework for R.}
\label{fig:shiny}
\end{figure}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Enabling future extensions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Since new methods are continually being developed (e.g., \citep{Zappia2018}), published benchmarks can quickly become out of date. To avoid this, it may be useful to design benchmarks so that they can be updated or extended at a later date, either by the original authors or other researchers. For example, creating publicly accessible repositories containing code and data allows researchers to extend the benchmark to include new methods or datasets, or to try different parameter settings or pre-processing procedures (e.g., \citep{Saelens2018a, Saelens2018b, Duo2018, Soneson2018, Weber2016}). New results can be published as an online supplement or as updates to an interactive website.

In addition to raw data and code, it is also useful to provide pre-processed data and results (e.g., \citep{Duo2018}), especially for computationally intensive benchmarks. This allows method developers to compare new methods more easily, since they can directly add a new method to the benchmark. This may be combined with an interactive website, where users can upload results from a new method, automatically generating an updated comparison (e.g., \citep{Kanitz2015}).




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Reproducible research best practices}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Reproducibility of research findings has become an increasing concern in numerous areas of research \citep{Ioannidis2005}. In computational sciences, reproducibility of code scripts and data analyses has been recognized as a useful `minimum standard' that enables other researchers to verify analyses \citep{Peng2011}. This can be achieved by making all code and data (where permitted) available via public repositories. In the context of benchmarking, this also provides further benefits for both method users and developers. For method users, public code repositories serve as a source of annotated code to run methods and build analysis pipelines, which is especially valuable for users without extensive computational experience. For developers, code repositories can act as a prototype for future method development work as well as extensions to the benchmark.

Many free online tools are available for these purposes, including GitHub for code sharing, publicly funded data repositories for specific data types (including ArrayExpress \citep{Kolesnikov2015}, the Gene Expression Omnibus \citep{Barrett2013}, and FlowRepository \citep{Spidlen2012}), and more general data repositories including Bioconductor's ExperimentHub, figshare, and Zenodo. Custom data repositories (e.g., \citep{Soneson2018, Soneson2016}) can be designed when additional flexibility is needed. Several repositories allow the creation of `digital object identifiers' (DOIs) for code or data objects, which can be used for citation purposes. Workflow management tools (e.g., SummarizedBenchmark \citep{Kimes2018}, DataPackageR \citep{Finak2018}, workflowr \citep{Blischak2018}, Dynamic Statistical Comparisons \citep{Wang2018}, and Snakemake \citep{Koster2012}) are helpful for creating a complete record of steps in a benchmarking pipeline.

An extensive literature exists on best practices for computational research (e.g., \citep{Sandve2013}). For extensive benchmarks, additional best practices from software development are also useful, including the use of makefiles, unit testing, continuous integration, and containerization. While the optimal level of computational reproducibility will depend on the scope of the benchmark and level of computational expertise, in our experience, almost all efforts in this area prove highly useful, especially by facilitating later adaptations or extensions to the study by ourselves or other researchers.




\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Author contributions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

LMW proposed the project and drafted the manuscript. WS, RC, CS, YS, and MDR contributed ideas and references and contributed to drafting of the manuscript. YS and MDR supervised the project. All authors read and approved the final manuscript.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Competing interests}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The authors declare that they have no competing interests.




%%%%%%%%%%%%
% References
%%%%%%%%%%%%

\newpage


\singlespacing
\footnotesize

\bibliography{benchmarking_guidelines_paper}

\bibliographystyle{apa}




\end{document}



